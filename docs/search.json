[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jesus Gonzalez",
    "section": "",
    "text": "Data TPM | PlayStation\nMilitary Interview Coach | Candorful\nTransitioning Military Mentor | Veterati\n\n\n\n\n\n\nUS Air Force | 2015 - 2022\n\n\n\n\n\n\nMSBA | UCSD Rady 2024\nMBA | AMU 2022\nBBA | AMU 2020\nAAS | CCAF 2018\n\n\n\n\n\n\nPMP | PMI 2022\nPSM | Scrum.org 2022"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jesus Gonzalez",
    "section": "",
    "text": "Data TPM | PlayStation\nMilitary Interview Coach | Candorful\nTransitioning Military Mentor | Veterati\n\n\n\n\n\n\nUS Air Force | 2015 - 2022\n\n\n\n\n\n\nMSBA | UCSD Rady 2024\nMBA | AMU 2022\nBBA | AMU 2020\nAAS | CCAF 2018\n\n\n\n\n\n\nPMP | PMI 2022\nPSM | Scrum.org 2022\nWebsite source code here"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Jesus Gonzalez’s Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Key Drivers Analysis\n\n\n\n\n\n\nJesus Gonzalez\n\n\nMay 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Examples\n\n\n\n\n\n\nJesus Gonzalez\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nJesus Gonzalez\n\n\nMay 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPlug and Play RAG LLM\n\n\n\n\n\n\nJesus Gonzalez\n\n\nApr 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nJesus Gonzalez\n\n\nApr 16, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Project Example 1",
    "section": "",
    "text": "Ploting some data for funzies.\n\n\nSome other data stuff.\n\n\nblah blah here\n\nbulleted item 1\nthe second whatever\n\n\nblah blah one\nblah two\nblah three\n\ntext can be bold or italics, or strikethrough\nmy website is https://rsm-jeg031.github.io/ or here"
  },
  {
    "objectID": "projects/project1/index.html#header-2",
    "href": "projects/project1/index.html#header-2",
    "title": "Project Example 1",
    "section": "",
    "text": "Some other data stuff."
  },
  {
    "objectID": "projects/project2/index.html",
    "href": "projects/project2/index.html",
    "title": "Project Example 2",
    "section": "",
    "text": "Ploting some data.\n\n\nSome other data stuff."
  },
  {
    "objectID": "projects/project2/index.html#header-2",
    "href": "projects/project2/index.html#header-2",
    "title": "Project Example 2",
    "section": "",
    "text": "Some other data stuff."
  },
  {
    "objectID": "projects/project1/index.html#sub-section-of-1-some-smaller-stuff-here",
    "href": "projects/project1/index.html#sub-section-of-1-some-smaller-stuff-here",
    "title": "Project Example 1",
    "section": "",
    "text": "Some other data stuff.\n\n\nblah blah here\n\nbulleted item 1\nthe second whatever\n\n\nblah blah one\nblah two\nblah three\n\ntext can be bold or italics, or strikethrough\nmy website is https://rsm-jeg031.github.io/ or here"
  },
  {
    "objectID": "projects/Assignment1/hw1_questions.html",
    "href": "projects/Assignment1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experimental design varied the matching ratios, maximum amount of leadership donation matching, and suggested donation amounts. The results revealed that the presence of a matching grant notably increased both the revenues per solicitation by 19% and the probability of an individual donating by 22%. The researchers concluded that the presence of a match likely serves as a quality signal or timing signal that can effectiely increase donations, especialy among demographics and regions more responsive to such cues. Their finding suggested practical implications for the design of fundraising campaigns and provide avenues for further theoretical research on charitable givings.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/Assignment1/hw1_questions.html#introduction",
    "href": "projects/Assignment1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experimental design varied the matching ratios, maximum amount of leadership donation matching, and suggested donation amounts. The results revealed that the presence of a matching grant notably increased both the revenues per solicitation by 19% and the probability of an individual donating by 22%. The researchers concluded that the presence of a match likely serves as a quality signal or timing signal that can effectiely increase donations, especialy among demographics and regions more responsive to such cues. Their finding suggested practical implications for the design of fundraising campaigns and provide avenues for further theoretical research on charitable givings.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/Assignment1/hw1_questions.html#data",
    "href": "projects/Assignment1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset contains 50,083 entries and 51 columns, each with varying data types.\nHere are some key points:\n\ntreatment and control columns separate the groups for experimentation.\nThe ratio variables (ratio, ratio2, ratio3), size variables (size25, size50, size100, sizeno), and ask variables (ask, askd1, askd2, askd3, ask1, ask2, ask3), refer to the different experimental conditions.\nThe amount and amountchange columns measure impact.\nThe years, female, couple, nonlit, cases, and geographic variables like statecnt, stateresponse, provide demographic and location-based contextual data.\n\n\n\n\n\n\n\nInteractive Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\nask\naskd1\naskd2\naskd3\nask1\nask2\nask3\namount\ngave\namountchange\nhpa\nltmedmra\nfreq\nyears\nyear5\nmrm2\ndormant\nfemale\ncouple\nstate50one\nnonlit\ncases\nstatecnt\nstateresponse\nstateresponset\nstateresponsec\nstateresponsetminc\nperbush\nclose25\nred0\nblue0\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\nLoading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nT-Test\nComparing the means of variables like mrm2 for the treatment and control groups allows us to see if there’s any obvious statistically significantly different groups at a 95% confidence level.\n\nt_test_treatment_control, t_test_control\n\n(TtestResult(statistic=0.1194921058159193, pvalue=0.9048859731777738, df=50080.0),\n TtestResult(statistic=0.0, pvalue=1.0, df=33372.0))\n\n\nA quick pass at a variable last ‘month since last donation’ tells us there is no significant different between treatment and control groups for this value. Both samples are similar enough.\nBelow I tested the dormant variable leading to similar results, pointing to an unbaised dataset.\n\nt_test_dormant_treatment_control\n\nTtestResult(statistic=0.17388504815227449, pvalue=0.8619565062750842, df=50081.0)\n\n\nRegression Coefficients\nExplanatory variables used:\n\nfemale\ncouple\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\nlin_reg_dormant.summary()\n\nLinear regression (OLS)\nData                 : Treatment Group\nResponse variable    : dormant\nExplanatory variables: female, couple, pwhite, pblack, page18_39, ave_hh_sz, median_hhincome, powner, psch_atlstba, pop_propurban\nNull hyp.: the effect of x on dormant is zero\nAlt. hyp.: the effect of x on dormant is not zero\n\n                 coefficient  std.error  t.value p.value     \nIntercept              0.552      0.053   10.411  &lt; .001  ***\nfemale                 0.001      0.006    0.105   0.916     \ncouple                -0.051      0.010   -5.141  &lt; .001  ***\npwhite                -0.029      0.041   -0.711   0.477     \npblack                 0.049      0.040    1.225   0.221     \npage18_39              0.081      0.046    1.762   0.078    .\nave_hh_sz             -0.002      0.013   -0.118   0.906     \nmedian_hhincome        0.000      0.000    1.939   0.052    .\npowner                -0.019      0.033   -0.572   0.568     \npsch_atlstba          -0.045      0.033   -1.362   0.173     \npop_propurban         -0.032      0.013   -2.503   0.012    *\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.002, Adjusted R-squared: 0.001\nF-statistic: 5.652 df(10, 31296), p.value &lt; 0.001\nNr obs: 31,307\n\n\n\nlin_reg_dormant.plot(\"vimp\")\n\n\n\n\n\n\n\n\n\nlin_reg_dormant_2.summary()\n\nLinear regression (OLS)\nData                 : Control Group\nResponse variable    : dormant\nExplanatory variables: female, couple, pwhite, pblack, page18_39, ave_hh_sz, median_hhincome, powner, psch_atlstba, pop_propurban\nNull hyp.: the effect of x on dormant is zero\nAlt. hyp.: the effect of x on dormant is not zero\n\n                 coefficient  std.error  t.value p.value     \nIntercept              0.453      0.075    6.073  &lt; .001  ***\nfemale                -0.008      0.009   -0.862   0.389     \ncouple                -0.039      0.014   -2.743   0.006   **\npwhite                 0.160      0.057    2.780   0.005   **\npblack                 0.142      0.056    2.546   0.011    *\npage18_39             -0.001      0.065   -0.011   0.991     \nave_hh_sz              0.012      0.019    0.637   0.524     \nmedian_hhincome        0.000      0.000    1.432   0.152     \npowner                -0.133      0.047   -2.857   0.004   **\npsch_atlstba          -0.070      0.047   -1.493   0.135     \npop_propurban         -0.016      0.018   -0.882   0.378     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.002, Adjusted R-squared: 0.001\nF-statistic: 2.579 df(10, 15634), p.value 0.004\nNr obs: 15,645\n\n\n\nlin_reg_dormant_2.plot(\"vimp\")\n\n\n\n\n\n\n\n\nThe significance of the coefficients and their changes between the two groups (treatment and control of the same variable) carry important implications for understanding the dynamics of how treaments affects various populations. Primarily the variable means are consistent between the groups, but impacts the them differently."
  },
  {
    "objectID": "projects/Assignment1/hw1_questions.html#experimental-results",
    "href": "projects/Assignment1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\n\n\n\n\n\n\n\n\n\n(TtestResult(statistic=3.101361000543946, pvalue=0.0019274025949016988, df=50081.0),\n &lt;class 'statsmodels.iolib.summary.Summary'&gt;\n \"\"\"\n                             OLS Regression Results                            \n ==============================================================================\n Dep. Variable:                   gave   R-squared:                       0.000\n Model:                            OLS   Adj. R-squared:                  0.000\n Method:                 Least Squares   F-statistic:                     9.618\n Date:                Tue, 23 Apr 2024   Prob (F-statistic):            0.00193\n Time:                        10:30:46   Log-Likelihood:                 26630.\n No. Observations:               50083   AIC:                        -5.326e+04\n Df Residuals:                   50081   BIC:                        -5.324e+04\n Df Model:                           1                                         \n Covariance Type:            nonrobust                                         \n ==============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n ------------------------------------------------------------------------------\n const          0.0179      0.001     16.225      0.000       0.016       0.020\n treatment      0.0042      0.001      3.101      0.002       0.002       0.007\n ==============================================================================\n Omnibus:                    59814.280   Durbin-Watson:                   2.005\n Prob(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\n Skew:                           6.740   Prob(JB):                         0.00\n Kurtosis:                      46.440   Cond. No.                         3.23\n ==============================================================================\n \n Notes:\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n \"\"\")\n\n\nStatistical Findings:\n\nT-test: The t-test results show a statistically significant difference with a p-value of approximately 0.0019. This indicates that the observed difference in donation rates between the treatment and control groups is unlikely to be due to chance.\nLinear Regression: The regression analysis confirms this finding. The coefficient for the treatment variable is positive (approximately 0.0042) and significant at the 0.002 level. This means that being in the treatment group increases the likelihood of donating by about 0.42 percentage points, holding other factors constant.\n\nInterpretation in Context of the Experiment: The statistical tests confirm that the treatment, which typically involves some form of intervention such as an enhanced fundraising appeal or incentive (like matching donations), has a positive effect on the probability of making a donation. This finding suggests that interventions designed to make giving more appealing or rewarding can indeed increase charitable contributions.\nImplications for Human Behavior: This outcome reveals insights into human behavior, especially in the context of philanthropy. The effectiveness of the treatment suggests that individuals are responsive to incentives or enhancements in the solicitation process. Essentially, when people perceive that their contributions will have a greater impact (such as through matching), they are more likely to contribute. This aligns with broader behavioral economics principles, which assert that people’s actions are often influenced by contextual cues and perceived benefits.\nThese results underline the importance of strategically designed fundraising campaigns that leverage psychological and economic incentives to boost charitable giving. By understanding and implementing what motivates people to give, nonprofits can more effectively mobilize resources to address various social issues.\n\n\nOptimization terminated successfully.\n         Current function value: 0.301543\n         Iterations 7\n\n\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n50083\n\n\nModel:\nProbit\nDf Residuals:\n50082\n\n\nMethod:\nMLE\nDf Model:\n0\n\n\nDate:\nTue, 23 Apr 2024\nPseudo R-squ.:\n-1.999\n\n\nTime:\n10:30:46\nLog-Likelihood:\n-15102.\n\n\nconverged:\nTrue\nLL-Null:\n-5035.4\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\nnan\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\ntreatment\n-2.0134\n0.015\n-131.734\n0.000\n-2.043\n-1.983\n\n\n\n\n\n\nComparison to Table 3, Column 1:\n\nThe paper reports that the treatment has a statistically significant effect on the likelihood of donating.\nThe results obtained here match in terms of significance and direction, even if the exact coefficients might differ due to the differences in the model specification (such as inclusion of constants and other controls that were not replicated exactly in this model)\n\nThis model suggests that assignment to the treatment does increase the likelihood of making a donation, aligning with findings in the paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\n(TtestResult(statistic=-0.96504713432247, pvalue=0.33453168549723933, df=22265.0),\n TtestResult(statistic=-1.0150255853798622, pvalue=0.3101046637086672, df=22260.0),\n TtestResult(statistic=-0.05011583793874515, pvalue=0.9600305283739325, df=22261.0))\n\n\n\n1:1 vs. 2:1 Match Ratio:\n\nStatistic: -0.965\nP-value: 0.335\nInterpretation: There is no statistically significant difference in donation rates between the 1:1 and 2:1 match ratios.\n\n1:1 vs. 3:1 Match Ratio:\n\nStatistic: -1.015\nP-value: 0.310\nInterpretation: There is no statistically significant difference in donation rates between the 1:1 and 3:1 match ratios.\n\n2:1 vs. 3:1 Match Ratio:\n\nStatistic: -0.050\nP-value: 0.960\nInterpretation: There is no statistically significant difference in donation rates between the 2:1 and 3:1 match ratios.\n\n\nInterpretation: The authors comment that the “figures suggest” certain outcomes regarding the match ratios, likely pointing towards expectations of different match ratios having varying effects on donation behavior. However, the t-test results indicate that there is no statistically significant difference in the likelihood of donating between any of the match ratios tested (1:1, 2:1, 3:1).\nThis suggests that increasing the match ratio, within the ranges tested, does not significantly influence the decision to donate in this particular data set and experimental setup. This finding is important because it challenges the assumption that simply increasing the match ratio will lead to higher donation rates, and supports a more nuanced view of how incentives impact charitable giving. It suggests that other factors beyond the match ratio might play more significant roles in influencing donor behavior.\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.665\nDate:                Tue, 23 Apr 2024   Prob (F-statistic):             0.0118\nTime:                        10:30:46   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50079   BIC:                        -5.322e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0207      0.001     15.398      0.000       0.018       0.023\n2              0.0019      0.002      0.989      0.323      -0.002       0.006\n3              0.0020      0.002      1.041      0.298      -0.002       0.006\nControl       -0.0029      0.002     -1.661      0.097      -0.006       0.001\n==============================================================================\nOmnibus:                    59812.754   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4316693.217\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.438   Cond. No.                         5.09\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nEffectiveness of Match Ratios:\n\nThe coefficients for the 2:1 and 3:1 match ratios (labeled as ‘2’ and ‘3’ in your regression) are not statistically significant. This indicates that, within this sample and under the conditions studied, increasing the match ratio from the baseline (possibly the 1:1 ratio or no match scenario) did not significantly increase the likelihood of donations.\nThis suggests that the psychological or motivational impact of these match ratios on donation behavior may be less pronounced than hypothesized or varies based on other unaccounted factors such as the demographic characteristics of donors, the context of the donation appeal, or the specific charitable cause.\n\nStatistical Precision and Significance:\n\nThe p-values associated with the 2:1 and 3:1 match ratios being above conventional significance levels (0.05) imply that the results could be due to random chance rather than a true effect of the match ratios.\nThe lack of significant findings here contrasts with common fundraising strategies that assume higher match ratios will universally boost donation rates. This could indicate that other elements of the campaign or external factors have more influence on the donation decision than the match ratio alone.\n\nModel Fit and Reliability:\n\nThe very low R-squared value suggests that the model does not explain much of the variance in donation behavior, indicating that other variables not included in the model might be influencing whether individuals decide to donate.\nThis points to a potential oversimplification in the model or the need to explore other factors that impact donation behavior, such as personal connection to the cause, previous donation behavior, economic conditions, or the manner in which the donation appeal is made.\n\nImplications for Future Research and Practice:\n\nThese results suggest that fundraisers and researchers should consider a broader range of factors when designing donation strategies and studies. It may not be sufficient to adjust the match ratio; instead, understanding the donor audience and tailoring the message might be more effective.\nFurther research could explore combinations of strategies, such as varying the communication style, the visibility of donation impacts, or combining match offers with other incentives.\n\n\nFrom Data\n\n\nDifference in donation rates between 1:1 and 2:1 ratios: 0.0018842510217149944\nDifference in donation rates between 2:1 and 3:1 ratios: 0.00010002398025293902\n\n\nFrom Fitted Coefficients\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     4.117\nDate:                Tue, 23 Apr 2024   Prob (F-statistic):             0.0163\nTime:                        10:30:46   Log-Likelihood:                 26629.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50080   BIC:                        -5.323e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0190      0.001     22.306      0.000       0.017       0.021\nratio2         0.0036      0.002      2.269      0.023       0.000       0.007\nratio3         0.0037      0.002      2.332      0.020       0.001       0.007\n==============================================================================\nOmnibus:                    59815.856   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317637.927\nSkew:                           6.741   Prob(JB):                         0.00\nKurtosis:                      46.443   Cond. No.                         3.16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nDifference in coefficients between 2:1 and 3:1 ratios: 0.00010002398025543442\n\n\nRegression Results Overview\n\nDependent Variable (gave): Indicates whether a donation was made.\nCoefficients:\n\nconst (0.0190): Baseline probability of making a donation, likely reflecting the control group or a 1:1 match ratio group.\nratio2 (0.0036): Increases the likelihood of donating by 0.36 percentage points, significant at the 0.023 level.\nratio3 (0.0037): Increases the likelihood of donating by 0.37 percentage points, significant at the 0.020 level.\n\n\nStatistical Significance\n\nratio2 and ratio3 are statistically significant, indicating that higher match ratios positively affect donation likelihood.\n\nComparing Match Ratios\n\nThe difference between ratio2 and ratio3 is 0.0001, suggesting no significant benefit in increasing the match ratio from 2:1 to 3:1.\n\nImplications for Fundraising Strategy\n\nBoth match ratios increase donation probabilities similarly; hence, moving from a 2:1 to a 3:1 match ratio does not yield significantly higher donations. This suggests a diminishing return on higher match ratios, guiding organizations to potentially optimize fundraising efforts without increasing the match offer.\nMatch incentives effectively increase donations, but overly generous matches may not provide additional benefits, supporting cost-effective fundraising strategies.\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n\nT-Statistic: 1.9182618934467577\nP-Value: 0.05508566528918335\n\n\nInterpretation: The initial analysis shows a T-Statistic close to 2, suggesting a moderate difference in donation amounts between the treatment and control groups across all individuals, including non-donors. The p-value is slightly above the conventional threshold of 0.05, indicating that this difference is not statistically significant at the 5% level. This result suggests that while there is a trend towards higher donations in the treatment group, we cannot confidently assert that the treatment has a statistically significant impact on donation amounts when considering the entire sample.\n\n\nT-Statistic: -0.5846089794983359\nP-Value: 0.5590471865673547\n\n\nInterpretation: When focusing only on individuals who made a donation, the T-Statistic is negative, indicating that the average donation amount in the treatment group might actually be lower than in the control group among donors, although the difference is small. The p-value is well above 0.05, suggesting that this observed difference is not statistically significant. This result indicates that among those who chose to donate, the treatment does not significantly influence the amount donated, implying that other factors may play a more critical role in determining donation size among this group.\nBoth analyses suggest that the treatment—while potentially affecting the decision to donate when considering the full dataset—does not have a significant impact on the amount donated, particularly among those who have already decided to donate. These findings underscore the complexity of donor behavior and suggest that the treatment may not be as effective in increasing donation amounts as it might be in influencing the decision to donate.\n\n\n\n\n\n\n\n\n\nRegression Results Overview\n\nDependent Variable (gave): Indicates whether a donation was made.\nCoefficients:\n\nconst (0.0190): Baseline probability of making a donation, likely reflecting the control group or a 1:1 match ratio group.\nratio2 (0.0036): Increases the likelihood of donating by 0.36 percentage points, significant at the 0.023 level.\nratio3 (0.0037): Increases the likelihood of donating by 0.37 percentage points, significant at the 0.020 level.\n\n\nStatistical Significance\n\nratio2 and ratio3 are statistically significant, indicating that higher match ratios positively affect donation likelihood.\n\nComparing Match Ratios\n\nThe difference between ratio2 and ratio3 is 0.0001, suggesting no significant benefit in increasing the match ratio from 2:1 to 3:1.\n\nImplications for Fundraising Strategy\n\nBoth match ratios increase donation probabilities similarly; hence, moving from a 2:1 to a 3:1 match ratio does not yield significantly higher donations. This suggests a diminishing return on higher match ratios, guiding organizations to potentially optimize fundraising efforts without increasing the match offer.\nMatch incentives effectively increase donations, but overly generous matches may not provide additional benefits, supporting cost-effective fundraising strategies."
  },
  {
    "objectID": "projects/Assignment1/hw1_questions.html#simulation-experiment",
    "href": "projects/Assignment1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem. Suppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. Further suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\n\n\n\n\n\n\n\n\nThe plot indicates that, as the number of simulations increases, the cumulative average of the difference fluctuates around the true difference but does not consistently converge to the exact value of 0.004. This could be due to randomness inherent in the simulation of Bernoulli trials.\nDespite the fluctuations, the cumulative average does seem to stabilize as the number of simulations grows, which is consistent with the Law of Large Numbers. This law states that as the number of trials increases, the sample mean will get closer to the expected value.\nHowever, given that the cumulative average does not settle precisely on the true difference but hovers around it, this illustrates the role of variability when working with probabilities and random processes. The Central Limit Theorem would predict that the distribution of the sample means (if we repeated this entire process many times) would form a normal distribution centered around the true difference, with the variance of that distribution decreasing with more trials.\n\nCumulative verages approaching true difference in means The output is expected in the sense that it reflects the behavior described by the Law of Large Numbers, where the sample average will tend to get closer to the population mean with a large number of trials. Overall, the plot demonstrates that the cumulative average of the differences between the treatment and control probabilities does trend towards the true difference, validating the Law of Large Numbers.\n\n\nCentral Limit Theorem\n\n\n\n\n\n\n\n\n\nThe histograms depict the distribution of average differences in donation probabilities between treatment and control groups across different sample sizes, demonstrating the Central Limit Theorem (CLT).\nInterpretation within the Study Context:\n\nSample Size: 50\n\nThe distribution is somewhat bell-shaped but shows considerable spread and variability around the true mean difference. The red dashed line, which represents the true mean difference of 0.004, does not appear to be in the exact center of the distribution, indicating the impact of higher variability at lower sample sizes.\n\nSample Size: 200\n\nThe distribution becomes more bell-shaped and starts to center around the true mean difference. The variability is reduced compared to a sample size of 50, which is expected as per the CLT.\n\nSample Size: 500\n\nFurther narrowing and centering around the true difference are observed, with the distribution taking a more definitive normal shape.\n\nSample Size: 1000\n\nThe histogram for a sample size of 1000 shows the distribution closely centered around the true mean difference, with even less variability, demonstrating the CLT’s prediction of a normal distribution as sample size increases.\n\n\nCentral Limit Theorem Validation:\n\nAcross all histograms, zero is not in the exact center because the true mean difference is not zero; it’s 0.004. The green dashed line, which would represent zero, is clearly not in the center, especially for larger sample sizes. Instead, it falls within the left tail of the distributions, particularly for sample sizes of 500 and 1000, where the bell shape is more apparent.\nAs sample sizes increase, the distribution of the average differences increasingly conforms to a normal distribution centered around the true difference (0.004), not zero, validating the CLT.\n\nThe histograms support the CLT’s assertion that with larger sample sizes, the sampling distribution of the mean will approximate a normal distribution centered around the true population mean. Zero is not in the center but rather in the tail of the distribution, as the treatment and control groups’ donation probabilities differ by 0.004.\nIn the context of the study, these results imply that with sufficient sample sizes, any difference in means due to random chance (sampling variability) will average out, and we can expect the sample mean to reliably estimate the population mean difference. This is critical for replication analysis, as it underlines the importance of sample size in detecting true effects and ensuring that findings are not artifacts of random variation. The CLT allows researchers to make inferences about population parameters based on sample statistics, a foundational concept in hypothesis testing and confidence interval estimation."
  },
  {
    "objectID": "projects/Example/index.html",
    "href": "projects/Example/index.html",
    "title": "Sample Section",
    "section": "",
    "text": "Ploting some data for funzies. And playing around with Quarto website capabilities.\n\n\nSome other data stuff.\n\n\nblah blah here\n\nbulleted item 1\nthe second whatever\n\n\nblah blah one\nblah two\nblah three\n\ntext can be bold or italics, or strikethrough\nmy website is https://rsm-jeg031.github.io/ or here"
  },
  {
    "objectID": "projects/Example/index.html#sub-section-of-1-some-smaller-stuff-here",
    "href": "projects/Example/index.html#sub-section-of-1-some-smaller-stuff-here",
    "title": "Sample Section",
    "section": "",
    "text": "Some other data stuff.\n\n\nblah blah here\n\nbulleted item 1\nthe second whatever\n\n\nblah blah one\nblah two\nblah three\n\ntext can be bold or italics, or strikethrough\nmy website is https://rsm-jeg031.github.io/ or here"
  },
  {
    "objectID": "projects/Assignment1/hw1_questions.html#experimental-results-will-continue-later",
    "href": "projects/Assignment1/hw1_questions.html#experimental-results-will-continue-later",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results 🚩🚩🚩🚩 will continue later 🥱",
    "text": "Experimental Results 🚩🚩🚩🚩 will continue later 🥱\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "index.html#current",
    "href": "index.html#current",
    "title": "Jesus Gonzalez",
    "section": "",
    "text": "Data TPM | PlayStation\nMilitary Interview Coach | Candorful\nTransitioning Military Mentor | Veterati"
  },
  {
    "objectID": "index.html#work-history",
    "href": "index.html#work-history",
    "title": "Jesus Gonzalez",
    "section": "",
    "text": "US Air Force | 2015 - 2022"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jesus Gonzalez",
    "section": "",
    "text": "MSBA | UCSD Rady 2024\nMBA | AMU 2022\nBBA | AMU 2020\nAAS | CCAF 2018"
  },
  {
    "objectID": "index.html#certifications",
    "href": "index.html#certifications",
    "title": "Jesus Gonzalez",
    "section": "",
    "text": "PMP | PMI 2022\nPSM | Scrum.org 2022"
  },
  {
    "objectID": "about.html#current",
    "href": "about.html#current",
    "title": "Jesus Gonzalez",
    "section": "",
    "text": "Data TPM | PlayStation\nMilitary Interview Coach | Candorful\nTransitioning Military Mentor | Veterati"
  },
  {
    "objectID": "about.html#work-history",
    "href": "about.html#work-history",
    "title": "Jesus Gonzalez",
    "section": "",
    "text": "US Air Force | 2015 - 2022"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Jesus Gonzalez",
    "section": "",
    "text": "MSBA | UCSD Rady 2024\nMBA | AMU 2022\nBBA | AMU 2020\nAAS | CCAF 2018"
  },
  {
    "objectID": "about.html#certifications",
    "href": "about.html#certifications",
    "title": "Jesus Gonzalez",
    "section": "",
    "text": "PMP | PMI 2022\nPSM | Scrum.org 2022"
  },
  {
    "objectID": "projects/RAG LLM/rag-llm.html",
    "href": "projects/RAG LLM/rag-llm.html",
    "title": "Plug and Play RAG LLM",
    "section": "",
    "text": "Here’s an image GPT tried really hard to create!\nThis document demonstrate a quick and easy RAG implementation using OPENAIs API and the LangChain framework. I tried to make this a simple copy/pastable PDF solution for anyone looking to quickly interact with their PDFs or simply a large corpus of text."
  },
  {
    "objectID": "projects/RAG LLM/rag-llm.html#the-model",
    "href": "projects/RAG LLM/rag-llm.html#the-model",
    "title": "Plug and Play RAG LLM",
    "section": "The Model",
    "text": "The Model\n\nmodel = ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\nparser = StrOutputParser()\nchain = model | parser\nchain.invoke(\"hi, are you there gpt?\")\n\n\"Hello! Yes, I'm here. How can I assist you today?\""
  },
  {
    "objectID": "projects/RAG LLM/rag-llm.html#plug-play-section",
    "href": "projects/RAG LLM/rag-llm.html#plug-play-section",
    "title": "Plug and Play RAG LLM",
    "section": "Plug & Play Section",
    "text": "Plug & Play Section\n\ndef pdf_to_text(pdf_path, output_txt_path):\n    text = extract_text(pdf_path)\n    with open(output_txt_path, 'w', encoding='utf-8') as file:\n        file.write(text)\n\n# pdf_path = '../../files/Computer Age Statistical Inference Book.pdf'\n# output_txt_path = '../../files/rag-llm/Computer Age Statistical Inference Book.txt'  \n# pdf_to_text(pdf_path, output_txt_path)\n\nI am showing the short script above so that anyone may replicate this in their environment. I used a course textbook as a test case."
  },
  {
    "objectID": "projects/RAG LLM/rag-llm.html#chunking-context-due-to-token-limitations",
    "href": "projects/RAG LLM/rag-llm.html#chunking-context-due-to-token-limitations",
    "title": "Plug and Play RAG LLM",
    "section": "Chunking Context Due to Token Limitations",
    "text": "Chunking Context Due to Token Limitations\n\nloader = TextLoader(\"../../files/rag-llm/Computer Age Statistical Inference Book.txt\")\ntext_documents = loader.load()\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\ndocuments = text_splitter.split_documents(text_documents)"
  },
  {
    "objectID": "projects/RAG LLM/rag-llm.html#embedding-the-context-for-improved-performance",
    "href": "projects/RAG LLM/rag-llm.html#embedding-the-context-for-improved-performance",
    "title": "Plug and Play RAG LLM",
    "section": "Embedding the Context for Improved Performance",
    "text": "Embedding the Context for Improved Performance\n\nembeddings = OpenAIEmbeddings()\nvector_store = DocArrayInMemorySearch.from_documents(documents, embeddings)\n\nretriever = vector_store.as_retriever()\nretriever.invoke(\"Who or what do frequentists criticize?\") \n\n[Document(page_content='What might be called the strong deﬁnition of frequentism insists on exact\\nfrequentist correctness under experimental repetitions. Pivotality, unfortu-\\nnately, is unavailable in most statistical situations. Our looser deﬁnition\\nof frequentism, supplemented by devices such as those above,7 presents a\\nmore realistic picture of actual frequentist practice.\\n\\n2.2 Frequentist Optimality\\n\\nThe popularity of frequentist methods reﬂects their relatively modest math-\\nematical modeling assumptions: only a probability model F (more exactly\\na family of probabilities, Chapter 3) and an algorithm of choice t.x/. This\\nﬂexibility is also a defect in that the principle of frequentist correctness\\ndoesn’t help with the choice of algorithm. Should we use the sample mean\\nto estimate the location of the gfr distribution? Maybe the 25% Win-\\nsorized mean would be better, as Table 2.1 suggests.', metadata={'source': '../../files/rag-llm/Computer Age Statistical Inference Book.txt'}),\n Document(page_content='Frequentism cannot claim to be a seamless philosophy of statistical in-\\nference. Paradoxes and contradictions abound within its borders, as will\\nbe shown in the next chapter. That being said, frequentist methods have\\na natural appeal to working scientists, an impressive history of success-\\nful application, and, as our list of ﬁve “devices” suggests, the capacity to\\nencourage clever methodology. The story that follows is not one of aban-\\ndonment of frequentist thinking, but rather a broadening of connections\\nwith other methods.\\n\\n2.3 Notes and Details', metadata={'source': '../../files/rag-llm/Computer Age Statistical Inference Book.txt'}),\n Document(page_content='Despite its simplicity, or perhaps because of it, objective Bayes procedures\\nare vulnerable to criticism from both ends of the statistical spectrum. From\\nthe subjectivist point of view, objective Bayes is only partially Bayesian: it\\nemploys Bayes’ theorem but without doing the hard work of determining a\\nconvincing prior distribution. This introduces frequentist elements into its\\npractice—clearly so in the case of Jeffreys’ prior—along with frequentist\\nincoherencies.\\n\\nFor the frequentist, objective Bayes analysis can seem dangerously un-\\ntethered from the usual standards of accuracy, having only tenuous large-\\nsample claims to legitimacy. This is more than a theoretical objection. The\\npractical advantages claimed for Bayesian methods depend crucially on the\\nﬁne structure of the prior. Can we safely ignore stopping rules or selective\\ninference (e.g., choosing the largest of many estimated parameters for spe-\\ncial attention) for a prior not based on some form of genuine experience?', metadata={'source': '../../files/rag-llm/Computer Age Statistical Inference Book.txt'}),\n Document(page_content='Frequentist statistics has the advantage of being applicable to any algo-\\nrithmic procedure, for instance to our Cp/OLS estimator. This has great\\nappeal in an era of enormous data sets and fast computation. The draw-\\nback, compared with Bayesian statistics, is that we have no guarantee that\\nour chosen algorithm is best in any way. Classical statistics developed a\\ntheory of best for a catalog of comparatively simple estimation and testing\\nproblems. In this sense, modern inferential theory has not yet caught up\\nwith modern problems such as data-based model selection, though tech-\\nniques such as model averaging (e.g., bagging) suggest promising steps\\nforward.\\n\\n20.3 Selection Bias', metadata={'source': '../../files/rag-llm/Computer Age Statistical Inference Book.txt'})]\n\n\nThe PDF extract is not great … but you see the potential in cleaned text. Read the embedded text it considered most relevant to the question. This differentiates a ‘RAG’ that simply reads a file versus a RAG that passes the corpus through an embedding model."
  },
  {
    "objectID": "projects/RAG LLM/rag-llm.html#setup-chaining",
    "href": "projects/RAG LLM/rag-llm.html#setup-chaining",
    "title": "Plug and Play RAG LLM",
    "section": "Setup & Chaining",
    "text": "Setup & Chaining\n\ninstructions = \"\"\"\nAnswer the question based on the context below. Prior to finalizing your response, \nremember to clean the data and make sense of it. You are a pretrained LLM that understands\ncommon language, so use your best judgement if the text is too messy to give a definitive answer. \nIf you can't answer the question because the text is too messy,\nreply \"The text is too messy to answer this question\". If you can't answer the question in general, reply \"I don't know\". \n\nContext: {context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(instructions)\nsetup = RunnableParallel(context=retriever, question=RunnablePassthrough())\n\nchain = setup | prompt | model | parser\nchain.invoke(\"Who or what do frequentists criticize?\") \n\n'Frequentists criticize objective Bayes procedures.'\n\n\nLooks like neither of them were right…transformers for the win!"
  },
  {
    "objectID": "projects/RAG LLM/rag-llm.html#exploring-embeddings",
    "href": "projects/RAG LLM/rag-llm.html#exploring-embeddings",
    "title": "Plug and Play RAG LLM",
    "section": "Exploring Embeddings",
    "text": "Exploring Embeddings\nExploring embeddings using gensim.\n\nsentences = LineSentence('../../files/rag-llm/Computer Age Statistical Inference Book.txt')\nmodel = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=5, workers=4)\n\nSetting up the model by breaking up the book into sentence chunks.\n\nwords = [word for word, _ in model.wv.most_similar('statistics', topn=30)]\nword_vectors = [model.wv[word] for word in words]\npca = PCA(n_components=2)\nreduced_vectors = pca.fit_transform(word_vectors)\n\n\n\n\n\n\n\n\n\n\nA two-dimensional view of the most similar words, based on their embeddings, to the word “statistics.”\n\n\n\n\n\n\n\n\n\nA three-dimensional view of the same query.\n\nCosine Similarity\nHere you can see the Cosine similarity between any words of your choice. Given the context of the book and the questions asked above to the GPT-powered model I chose the following.\n\nwords = ['frequentist', 'Bayes', 'objective', 'subjective'] \n\n\n\nCosine similarity between frequentist and Bayes: 0.9953806400299072\nCosine similarity between objective and subjective: 0.9843490123748779\n\n\n\n\n\n\n\n\n\nThe placement of the words in space aligns with the context of the book."
  },
  {
    "objectID": "projects/RAG-LLM/rag-llm.html",
    "href": "projects/RAG-LLM/rag-llm.html",
    "title": "Plug and Play RAG LLM",
    "section": "",
    "text": "Here’s an image GPT tried really hard to create!\nThis document demonstrate a quick and easy RAG implementation using OPENAIs API and the LangChain framework. I tried to make this a simple copy/pastable PDF solution for anyone looking to quickly interact with their PDFs or simply a large corpus of text."
  },
  {
    "objectID": "projects/RAG-LLM/rag-llm.html#the-model",
    "href": "projects/RAG-LLM/rag-llm.html#the-model",
    "title": "Plug and Play RAG LLM",
    "section": "The Model",
    "text": "The Model\n\nmodel = ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\nparser = StrOutputParser()\nchain = model | parser\nchain.invoke(\"hi, are you there gpt?\")\n\n\"Hello! Yes, I'm here. How can I help you today?\""
  },
  {
    "objectID": "projects/RAG-LLM/rag-llm.html#plug-play-section",
    "href": "projects/RAG-LLM/rag-llm.html#plug-play-section",
    "title": "Plug and Play RAG LLM",
    "section": "Plug & Play Section",
    "text": "Plug & Play Section\n\ndef pdf_to_text(pdf_path, output_txt_path):\n    text = extract_text(pdf_path)\n    with open(output_txt_path, 'w', encoding='utf-8') as file:\n        file.write(text)\n\n# pdf_path = '../../files/Computer Age Statistical Inference Book.pdf'\n# output_txt_path = '../../files/rag-llm/Computer Age Statistical Inference Book.txt'  \n# pdf_to_text(pdf_path, output_txt_path)\n\nI am showing the short script above so that anyone may replicate this in their environment. I used a course textbook as a test case."
  },
  {
    "objectID": "projects/RAG-LLM/rag-llm.html#chunking-context-due-to-token-limitations",
    "href": "projects/RAG-LLM/rag-llm.html#chunking-context-due-to-token-limitations",
    "title": "Plug and Play RAG LLM",
    "section": "Chunking Context Due to Token Limitations",
    "text": "Chunking Context Due to Token Limitations\n\nloader = TextLoader(\"../../files/rag-llm/Computer Age Statistical Inference Book.txt\")\ntext_documents = loader.load()\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\ndocuments = text_splitter.split_documents(text_documents)"
  },
  {
    "objectID": "projects/RAG-LLM/rag-llm.html#embedding-the-context-for-improved-performance",
    "href": "projects/RAG-LLM/rag-llm.html#embedding-the-context-for-improved-performance",
    "title": "Plug and Play RAG LLM",
    "section": "Embedding the Context for Improved Performance",
    "text": "Embedding the Context for Improved Performance\n\nembeddings = OpenAIEmbeddings()\nvector_store = DocArrayInMemorySearch.from_documents(documents, embeddings)\n\nretriever = vector_store.as_retriever()\nretriever.invoke(\"Who or what do frequentists criticize?\") \n\n[Document(page_content='What might be called the strong deﬁnition of frequentism insists on exact\\nfrequentist correctness under experimental repetitions. Pivotality, unfortu-\\nnately, is unavailable in most statistical situations. Our looser deﬁnition\\nof frequentism, supplemented by devices such as those above,7 presents a\\nmore realistic picture of actual frequentist practice.\\n\\n2.2 Frequentist Optimality\\n\\nThe popularity of frequentist methods reﬂects their relatively modest math-\\nematical modeling assumptions: only a probability model F (more exactly\\na family of probabilities, Chapter 3) and an algorithm of choice t.x/. This\\nﬂexibility is also a defect in that the principle of frequentist correctness\\ndoesn’t help with the choice of algorithm. Should we use the sample mean\\nto estimate the location of the gfr distribution? Maybe the 25% Win-\\nsorized mean would be better, as Table 2.1 suggests.', metadata={'source': '../../files/rag-llm/Computer Age Statistical Inference Book.txt'}),\n Document(page_content='Frequentism cannot claim to be a seamless philosophy of statistical in-\\nference. Paradoxes and contradictions abound within its borders, as will\\nbe shown in the next chapter. That being said, frequentist methods have\\na natural appeal to working scientists, an impressive history of success-\\nful application, and, as our list of ﬁve “devices” suggests, the capacity to\\nencourage clever methodology. The story that follows is not one of aban-\\ndonment of frequentist thinking, but rather a broadening of connections\\nwith other methods.\\n\\n2.3 Notes and Details', metadata={'source': '../../files/rag-llm/Computer Age Statistical Inference Book.txt'}),\n Document(page_content='Despite its simplicity, or perhaps because of it, objective Bayes procedures\\nare vulnerable to criticism from both ends of the statistical spectrum. From\\nthe subjectivist point of view, objective Bayes is only partially Bayesian: it\\nemploys Bayes’ theorem but without doing the hard work of determining a\\nconvincing prior distribution. This introduces frequentist elements into its\\npractice—clearly so in the case of Jeffreys’ prior—along with frequentist\\nincoherencies.\\n\\nFor the frequentist, objective Bayes analysis can seem dangerously un-\\ntethered from the usual standards of accuracy, having only tenuous large-\\nsample claims to legitimacy. This is more than a theoretical objection. The\\npractical advantages claimed for Bayesian methods depend crucially on the\\nﬁne structure of the prior. Can we safely ignore stopping rules or selective\\ninference (e.g., choosing the largest of many estimated parameters for spe-\\ncial attention) for a prior not based on some form of genuine experience?', metadata={'source': '../../files/rag-llm/Computer Age Statistical Inference Book.txt'}),\n Document(page_content='Frequentist statistics has the advantage of being applicable to any algo-\\nrithmic procedure, for instance to our Cp/OLS estimator. This has great\\nappeal in an era of enormous data sets and fast computation. The draw-\\nback, compared with Bayesian statistics, is that we have no guarantee that\\nour chosen algorithm is best in any way. Classical statistics developed a\\ntheory of best for a catalog of comparatively simple estimation and testing\\nproblems. In this sense, modern inferential theory has not yet caught up\\nwith modern problems such as data-based model selection, though tech-\\nniques such as model averaging (e.g., bagging) suggest promising steps\\nforward.\\n\\n20.3 Selection Bias', metadata={'source': '../../files/rag-llm/Computer Age Statistical Inference Book.txt'})]\n\n\nThe PDF extract is not great … but you see the potential in cleaned text. Read the embedded text it considered most relevant to the question. This differentiates a ‘RAG’ that simply reads a file versus a RAG that passes the corpus through an embedding model."
  },
  {
    "objectID": "projects/RAG-LLM/rag-llm.html#setup-chaining",
    "href": "projects/RAG-LLM/rag-llm.html#setup-chaining",
    "title": "Plug and Play RAG LLM",
    "section": "Setup & Chaining",
    "text": "Setup & Chaining\n\ninstructions = \"\"\"\nAnswer the question based on the context below. Prior to finalizing your response, \nremember to clean the data and make sense of it. You are a pretrained LLM that understands\ncommon language, so use your best judgement if the text is too messy to give a definitive answer. \nIf you can't answer the question because the text is too messy,\nreply \"The text is too messy to answer this question\". If you can't answer the question in general, reply \"I don't know\". \n\nContext: {context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(instructions)\nsetup = RunnableParallel(context=retriever, question=RunnablePassthrough())\n\nchain = setup | prompt | model | parser\nchain.invoke(\"Who or what do frequentists criticize?\") \n\n'Frequentists criticize the objective Bayes procedures for being vulnerable to criticism from both subjectivist and frequentist perspectives.'\n\n\nLooks like neither of them were right…transformers for the win!"
  },
  {
    "objectID": "projects/RAG-LLM/rag-llm.html#exploring-embeddings",
    "href": "projects/RAG-LLM/rag-llm.html#exploring-embeddings",
    "title": "Plug and Play RAG LLM",
    "section": "Exploring Embeddings",
    "text": "Exploring Embeddings\nExploring embeddings using gensim.\n\nsentences = LineSentence('../../files/rag-llm/Computer Age Statistical Inference Book.txt')\nmodel = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=5, workers=4)\n\nSetting up the model by breaking up the book into sentence chunks.\n\nwords = [word for word, _ in model.wv.most_similar('statistics', topn=30)]\nword_vectors = [model.wv[word] for word in words]\npca = PCA(n_components=2)\nreduced_vectors = pca.fit_transform(word_vectors)\n\n\n\n\n\n\n\n\n\n\nA two-dimensional view of the most similar words, based on their embeddings, to the word “statistics.”\n\n\n\n\n\n\n\n\n\nA three-dimensional view of the same query.\n\nCosine Similarity\nHere you can see the Cosine similarity between any words of your choice. Given the context of the book and the questions asked above to the GPT-powered model I chose the following.\n\nwords = ['frequentist', 'Bayes', 'objective', 'subjective'] \n\n\n\nCosine similarity between frequentist and Bayes: 0.995741069316864\nCosine similarity between objective and subjective: 0.9796308875083923\n\n\n\n\n\n\n\n\n\nThe placement of the words in space aligns with the context of the book."
  },
  {
    "objectID": "projects/Assignment2/hw2_questions.html",
    "href": "projects/Assignment2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\nInteractive Blueprinty Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnnamed: 0\npatents\nregion\nage\niscustomer\n\n\n\n\nLoading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnnamed: 0\npatents\nage\niscustomer\n\n\n\n\ncount\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n\n\nmean\n750.500000\n3.684667\n26.357667\n0.131333\n\n\nstd\n433.157015\n2.352500\n7.242528\n0.337877\n\n\nmin\n1.000000\n0.000000\n9.000000\n0.000000\n\n\n25%\n375.750000\n2.000000\n21.000000\n0.000000\n\n\n50%\n750.500000\n3.000000\n26.000000\n0.000000\n\n\n75%\n1125.250000\n5.000000\n31.625000\n0.000000\n\n\nmax\n1500.000000\n16.000000\n49.000000\n1.000000\n\n\n\n\n\n\n\n\n\n\n\nHere’s a brief summary of the dataset:\n\npatents: Number of patents awarded over the last 5 years.\nregion: Regional location of the firm.\nage: Age of the firm since incorporation.\niscustomer: Indicates whether the firm uses Blueprinty’s software (1 = customer, 0 = not a customer)\n\nDistributions of Customers\n\n\n\n\n\n\n\n\n\nNon Customer Mean: 3.62 Customer Mean: 4.09\nFrom the histograms and mean calculations, it appears that customers of Blueprinty tend to have a slightly higher average number of patents compared to non-customers. This could suggest a positive impact of using Blueprinty’s software on patent success, though other factors like firm age or region might also influence these results.\nComparing Regions\n\n\n\n\n\n\n\n\n\n\nRegion: There does not seem to be a very distinct pattern that strongly differentiates the regional distribution of customers versus non-customers, although some regions might slightly favor customer presence.\nAge: Customers tend to be younger firms compared to non-customers. This might suggest that younger firms are more likely to adopt Blueprinty’s software, or it could reflect market penetration strategies targeted at newer firms.\n\n\n\n\nIn the case of modeling the number of patents with a Poisson distribution, we start with the assumption that the number of patents awarded (\\(Y\\)) to each firm follows a Poisson distribution. The Poisson distribution is commonly used to model counts of events happening independently over a fixed period or in a fixed area, which fits the context of counting patents awarded over a specific time.\nPoisson Probability Mass Function\nFor a random variable \\(Y\\) that follows a Poisson distribution with a rate parameter \\(\\lambda\\) (average rate of occurrence over an interval), the probability mass function (PMF) is given by: \\[\nf(Y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nWhere: - \\(Y = 0, 1, 2, \\dots\\) is the count of patents (i.e., the number of events).\n\n\\(\\lambda &gt; 0\\) is the mean number of patents awarded per firm over the interval (5 years, in this case).\n\\(e\\) is the base of the natural logarithm.\n\\(Y!\\) is the factorial of \\(Y\\).\n\nLikelihood Function\nThe likelihood function for the Poisson distribution, given a sample of \\(n\\) observed values \\(Y_1, Y_2, \\dots, Y_n\\), is the product of the individual probabilities for each observed value: \\[\nL(\\lambda | Y_1, Y_2, \\dots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThis function indicates how likely it is to observe this particular set of data as a function of \\(\\lambda\\). When we fit a model, we look for the \\(\\lambda\\) that maximizes this likelihood, known as the Maximum Likelihood Estimate (MLE).\nLog-Likelihood\nFor computational convenience, it is often easier to maximize the log of the likelihood because the logarithm is a monotonically increasing function. The log-likelihood of the Poisson distribution is: \\[\n\\log L(\\lambda | Y_1, Y_2, \\dots, Y_n) = \\sum_{i=1}^{n} \\left(-\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!)\\right)\n\\]\nMaximizing this log-likelihood function with respect to \\(\\lambda\\) gives us the MLE of \\(\\lambda\\), which is particularly straightforward in the case of the Poisson distribution since the derivative of the log-likelihood function leads to a simple solution.\nUsing the statsmodels module we can easily implement the above, while taking into account the iscustomer status as a predictor for \\(\\lambda\\).\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\npatents\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1498\n\n\nModel Family:\nPoisson\nDf Model:\n1\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3362.7\n\n\nDate:\nTue, 30 Apr 2024\nDeviance:\n2352.6\n\n\nTime:\n19:24:57\nPearson chi2:\n2.25e+03\n\n\nNo. Iterations:\n4\nPseudo R-squ. (CS):\n0.006565\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n1.2874\n0.015\n88.453\n0.000\n1.259\n1.316\n\n\niscustomer\n0.1215\n0.038\n3.189\n0.001\n0.047\n0.196\n\n\n\n\n\n\nModel Summary:\n\nLog-Likelihood: The log-likelihood of the model is -3362.7, indicating the fit of the model to the data under the log link function.\nPseudo R-squared: The Pseudo R-squared value is 0.006565, which suggests that the model explains a small fraction of the variability in the number of patents.\nCoefficients:\n\nConstant (Intercept): The coefficient for the intercept is 1.2874. This can be interpreted as the log of the expected count of patents for non-customers, which exponentiated gives about \\(e^{1.2874} \\approx 3.62\\) patents on average for non-customers.\nCustomer Status: The coefficient for iscustomer is 0.1215. This coefficient is positive and statistically significant (p-value = 0.001). It suggests that being a customer is associated with a higher rate of patents. Exponentiating this coefficient gives about \\(e^{0.1215} \\approx 1.13\\), which means customers see a 13% increase in the rate of patents relative to non-customers.\n\n\nThis model indicates that there is a statistically significant but relatively modest effect of using Blueprinty’s software on the number of patents received. Customers of Blueprinty, on average, receive about 13% more patents than non-customers, holding other factors constant.\nManual Implementation\nThe formula for the log-likelihood of the Poisson distribution is: \\[\n\\log L(\\lambda | Y) = \\sum_{i=1}^{n} \\left(-\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!)\\right)\n\\]\nIn Python this looks like this:\n-lambda * n + sum(Y_i * log(lambda)) - sum(log(Y_i!))\n\ndef poisson_loglikelihood(lambda_, Y):\n    \"\"\"\n    Calculate the log-likelihood for the Poisson distribution given a lambda and an array of observed counts Y.\n    \n    Args:\n    lambda_ (float): The rate parameter of the Poisson distribution.\n    Y (array-like): Observed counts.\n    \n    Returns:\n    float: The log-likelihood value.\n    \"\"\"\n    return -lambda_ * len(Y) + np.sum(Y * np.log(lambda_)) - np.sum(gammaln(Y + 1))\n\n\nlog_likelihood = poisson_loglikelihood(lambda_, Y)\nlog_likelihood\n\n-3399.0629915336285\n\n\nA log-likelihood of -3399.06 indicates the probability of observing the given patent count data under the assumed Poisson distribution with the computed \\(\\lambda\\). This number by itself is abstract, but it becomes meaningful when used to compare this model against others, such as models with additional variables or different distributions.\nPloting \\(\\lambda\\)\n\nlambda_range = np.linspace(0.5 * lambda_, 1.5 * lambda_, 400)\nlog_likelihood_values = [poisson_loglikelihood(l, Y) for l in lambda_range]\n\nUsing the function created we plot \\(\\lambda\\) on the horizontal axis and the log-likelihood on the vertical axis for the range of lambdas.\n\n\n\n\n\n\n\n\n\n\nCurve Shape: The plot shows a peak around the estimated \\(\\lambda\\) (marked by the red dashed line), which is expected as this value of \\(\\lambda\\) maximizes the log-likelihood according to our model fit. This is characteristic of the behavior of likelihood functions around their maximum values, confirming that our estimation procedure is finding a plausible maximum.\nSensitivity: The curve is relatively steep around the estimated \\(\\lambda\\), indicating that small deviations from this value result in a noticeable decrease in log-likelihood. This suggests that the data provides a clear indication of the rate of patents, supporting the value we estimated.\n\nThe peak of the log-likelihood function tells us the most likely rate of patent awards (given the model and data), with values on either side less likely to produce the observed data.\nExploring MLE of Lambda for a Possion Distribution\nTo do this, we need to differentiate the log-likelihood function with respect to \\(\\lambda\\) and set it equal to zero. The log-likelihood function, given observations \\(Y_1, Y_2, \\dots, Y_n\\), is:\n\\[\n\\log L(\\lambda | Y) = \\sum_{i=1}^{n} \\left(-\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!)\\right)\n\\]\nTaking the derivative of this function with respect to \\(\\lambda\\), we get:\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda | Y) = \\sum_{i=1}^{n} \\left(-1 + \\frac{Y_i}{\\lambda}\\right)\n\\]\nSetting this derivative equal to zero for maximization:\n\\[\n- n + \\sum_{i=1}^{n} \\frac{Y_i}{\\lambda} = 0\n\\]\n\\[\n\\sum_{i=1}^{n} \\frac{Y_i}{\\lambda} = n\n\\]\n\\[\n\\frac{\\sum_{i=1}^{n} Y_i}{\\lambda} = n\n\\]\n\\[\n\\lambda = \\frac{\\sum_{i=1}^{n} Y_i}{n}\n\\]\nThus, \\(\\lambda_{MLE} = \\bar{Y}\\), where \\(\\bar{Y}\\) is the sample mean of the observed values \\(Y\\). This result aligns with our intuition and theoretical understanding, as \\(\\lambda\\) in a Poisson distribution represents both the mean and the variance of the distribution.\n\nlambda_mle = Y.mean()\nlambda_mle\n\n3.6846666666666668\n\n\nThe Maximum Likelihood Estimate (MLE) for lambda based on the observed data is approximately 3.685. This is the average number of patents per firm over the last 5 years, which indeed serves as the MLE of lambda in our Poisson model. This result confirms our theoretical derivation and reflects the natural interpretation of lambda in the Poisson distribution as the average rate (or mean) of occurrence of the event (patents awarded).\nFinding the MLE with scipy.optimize\n\ndef poisson_loglikelihood_v2(lambda_, Y):\n    \"\"\"\n    Calculate the negative log-likelihood for the Poisson distribution given a lambda and an array of observed counts Y.\n    \n    Args:\n    lambda_ (float): The rate parameter of the Poisson distribution.\n    Y (array-like): Observed counts.\n    \n    Returns:\n    float: The negative log-likelihood value.\n    \"\"\"\n    if lambda_ &lt;= 0:  # To ensure lambda is positive\n        return np.inf\n    return -(-lambda_ * len(Y) + np.sum(Y * np.log(lambda_)) - np.sum(gammaln(Y + 1)))\n\nUpdated version of the function above to make the absolute positive.\n\nfrom scipy.optimize import minimize\nresult = minimize(poisson_loglikelihood_v2, initial_lambda, args=(Y,), method='L-BFGS-B', bounds=[(0.001, None)])\nresult\n\n  message: CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH\n  success: True\n   status: 0\n      fun: 3367.6837722351047\n        x: [ 3.685e+00]\n      nit: 8\n      jac: [-2.728e-04]\n     nfev: 18\n     njev: 9\n hess_inv: &lt;1x1 LbfgsInvHessProduct with dtype=float64&gt;\n\n\nUsing minimize() finds the MLE of the lambda parameter for the given poisson distribution that best fits the observed data.\n\nEstimated lambda: Approximately 3.685, which closely matches the sample mean we calculated manually and confirms our previous findings.\nOptimization Status: The process converged successfully, indicating that a minimum of the negative log-likelihood function was found effectively.\nFunction Evaluations: It took 18 function evaluations to reach convergence, showing an efficient optimization given the simplicity of the function.\nLog-Likelihood: The minimum value of the log-likelihood at the optimum lambda is about 3367.684.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    \"\"\"\n    Calculate the negative log-likelihood for the Poisson regression model given beta coefficients, observed counts Y, and covariate matrix X.\n    \n    Args:\n    beta (array-like): The coefficient vector for the covariates.\n    Y (array-like): Observed counts.\n    X (2D array-like): Covariate matrix where each row corresponds to an observation and each column to a covariate.\n    \n    Returns:\n    float: The negative log-likelihood value.\n    \"\"\"\n    lambda_i = np.exp(X @ beta)\n    log_likelihood = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n    return -log_likelihood\n\nThis updated function allows us to pass the covariate matrix X.\nBeta Coefficients: The estimated coefficients for the model are as follows: - Intercept: beta_0 approx 1.215\n\nAge: beta_1 approx 1.046\nAge Squared: beta_2 approx -1.141\nCustomer Status: beta_3 approx 0.118\nRegion Northeast: beta_4 approx 0.099\nRegion Northwest: beta_5 approx -0.020\nRegion South: beta_6 approx 0.057\nRegion Southwest: beta_7 approx 0.051\nAge Effects: The positive coefficient for age and the negative coefficient for age squared suggest a non-linear relationship where the number of patents increases with age to a point, after which it begins to decrease. This could represent a “peak productive period” for firms.\nCustomer Status: The positive coefficient for being a customer indicates that, all else equal, firms using Blueprinty’s software are expected to have a higher rate of patents.\nRegional Effects: The coefficients for the regions indicate slight variations in patent rates across different regions, with some regions showing slightly higher or lower rates than the baseline (Midwest, which was dropped due to one-hot encoding).\n\nThis model provides a richer understanding of the factors influencing patent rates across firms and confirms the utility of including additional covariates to model such outcomes more accurately.\nHessian of Poisson Model with Covariats\nThe Hessian matrix is a fundamental component in the inference of maximum likelihood estimators. By calculating the inverse of the Hessian matrix at the optimum (MLEs of the parameters), we obtain the covariance matrix of the estimates. The diagonal elements of this matrix provide the variances (and thus standard errors) of the parameter estimates.\nThe Hessian also helps confirm the stability and reliability of the optimization process. A well-behaved Hessian (i.e., positive definite at the optimum) suggests that the optimization found a true maximum of the likelihood function. This is crucial for decision making and making statistical inferences.\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStandard Error\nStandard Error (from Hessian)\n\n\n\n\nintercept\n1.215451\n31.305234\n0.036425\n\n\nage\n1.046434\n57.308666\n0.100487\n\n\nage_squared\n-1.140819\n57.254890\n0.102494\n\n\niscustomer\n0.118172\n10.565816\n0.038920\n\n\nregion_Northeast\n0.098560\n23.819037\n0.042007\n\n\nregion_Northwest\n-0.020052\n3.153481\n0.053782\n\n\nregion_South\n0.057158\n9.626092\n0.052675\n\n\nregion_Southwest\n0.051278\n13.110999\n0.047213\n\n\n\n\n\n\n\n\nCoefficient:\n\nDescription: This column shows the estimated values of the regression coefficients (()) for each predictor in the model. These values represent the log change in the expected count of patents for a one-unit increase in the predictor, holding other variables constant.\nInterpretation:\n\nPositive values indicate that an increase in the predictor is associated with an increase in the expected count of patents.\nNegative values indicate that an increase in the predictor leads to a decrease in the expected count of patents.\nFor binary predictors like iscustomer, the coefficient tells us the log change in the expected count of patents when the firm is a customer versus not a customer.\n\n\nInitial Standard Error:\n\nDescription: Initially, the standard errors were calculated from an approximation of the covariance matrix obtained indirectly through the optimization process.\nInterpretation: Standard errors measure the variability or uncertainty in the coefficient estimates. Larger standard errors suggest greater uncertainty about the coefficient estimate.\n\nStandard Error (from Hessian):\n\nDescription: These standard errors are calculated directly from the Hessian matrix of the negative log-likelihood function at the estimated coefficients. The Hessian provides a second-order approximation to the curvature of the likelihood surface at the optimum.\nInterpretation: Like the initial standard errors, these values measure the precision of the estimates. The smaller these values, the more confident we can be about the accuracy of the estimated coefficients.\n\nValidating Results with Statsmodels glm()\nUp to this point in the analysis, we took a “looking under the hood” approach to this case study. Using the open source statsmodels api we can easily summarize our work.\n\nModel Coefficients\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient (Manual)\nStandard Error (Manual)\nCoefficient (GLM)\nStandard Error (GLM)\n\n\n\n\nIntercept\n1.215\n0.036\n1.215\n0.036\n\n\nAge\n1.046\n0.100\n1.046\n0.100\n\n\nAge Squared\n-1.141\n0.102\n-1.141\n0.102\n\n\nIs Customer\n0.118\n0.039\n0.118\n0.039\n\n\nRegion Northeast\n0.099\n0.042\n0.099\n0.042\n\n\nRegion Northwest\n-0.020\n0.054\n-0.020\n0.054\n\n\nRegion South\n0.057\n0.053\n0.057\n0.053\n\n\nRegion Southwest\n0.051\n0.047\n0.051\n0.047\n\n\n\n\nConsistency: The coefficients and standard errors from the statsmodels.GLM function match exactly with those computed manually using the Hessian-based approach. This indicates the correctness of our manual calculations and the model setup.\nStatistical Significance: The z-values and P&gt;|z| columns from the statsmodels output provide tests for the significance of each predictor. Coefficients with a lower p-value are statistically significant at common significance levels (e.g., 0.05), confirming the impact of those variables on the response variable.\nAge and Age Squared: Both coefficients are statistically significant with expected signs, indicating a peak effect of age on patent production.\nIs Customer: The positive coefficient for this variable is statistically significant, indicating that customers of Blueprinty are likely to have higher patent counts.\nRegions: Differences among regions show varying levels of significance, with the Northeast being significant, suggesting regional variations in patent counts.\n\n\n\n\nEffect of Using Blueprinty’s Software:\n\nThe coefficient for the iscustomer variable is positive and statistically significant. This suggests that firms using Blueprinty’s software, on average, have higher rates of patent awards compared to those that do not use the software.\nQuantitatively, the coefficient value (approximately 0.118) indicates that being a customer of Blueprinty is associated with an increase in the expected log count of patents. This translates into an exponential effect, suggesting that customers see about a 12.5% increase in the expected number of patents ([exp(0.118) - 1] * 100%).\n\nAge of Firm:\n\nThe relationship between the age of a firm and its patent success is modeled as a quadratic function, with both age and age_squared being statistically significant. This indicates a peak effect where the number of patents increases with age up to a point, after which it starts to decline. This could reflect increasing expertise and resources over time, followed by a plateau or decline as firms age further.\n\nRegional Variations:\n\nThere are significant regional differences in patent success rates. For example, firms in the Northeast show a significantly higher rate of patent success compared to the baseline region (Midwest), suggesting regional disparities in innovation or business environments that could affect patent outcomes.\n\nConclusions:\n\nBlueprinty’s Software: The analysis strongly supports the marketing claim that Blueprinty’s software helps increase the success rate of patent applications. Firms using Blueprinty’s software are statistically more likely to have higher patent counts, which could be attributed to the efficiency and effectiveness of the software in managing patent applications.\nPolicy and Strategy Implications: For Blueprinty, these findings justify strategies aimed at expanding their customer base by highlighting the positive impact of their software on patent success. Additionally, they might consider tailored marketing strategies for different regions or age groups of firms based on the differential effects observed.\nLimitations and Further Research: While the results are promising, they are based on observational data, which can be prone to confounding factors not controlled in the model. Further research could involve more detailed data, potentially longitudinal studies, to better isolate the effect of the software from other factors. Additionally, exploring other variables like industry type, size of the firm, or specific features used in the software could provide deeper insights."
  },
  {
    "objectID": "projects/Assignment2/hw2_questions.html#blueprinty-case-study",
    "href": "projects/Assignment2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\nInteractive Blueprinty Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnnamed: 0\npatents\nregion\nage\niscustomer\n\n\n\n\nLoading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnnamed: 0\npatents\nage\niscustomer\n\n\n\n\ncount\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n\n\nmean\n750.500000\n3.684667\n26.357667\n0.131333\n\n\nstd\n433.157015\n2.352500\n7.242528\n0.337877\n\n\nmin\n1.000000\n0.000000\n9.000000\n0.000000\n\n\n25%\n375.750000\n2.000000\n21.000000\n0.000000\n\n\n50%\n750.500000\n3.000000\n26.000000\n0.000000\n\n\n75%\n1125.250000\n5.000000\n31.625000\n0.000000\n\n\nmax\n1500.000000\n16.000000\n49.000000\n1.000000\n\n\n\n\n\n\n\n\n\n\n\nHere’s a brief summary of the dataset:\n\npatents: Number of patents awarded over the last 5 years.\nregion: Regional location of the firm.\nage: Age of the firm since incorporation.\niscustomer: Indicates whether the firm uses Blueprinty’s software (1 = customer, 0 = not a customer)\n\nDistributions of Customers\n\n\n\n\n\n\n\n\n\nNon Customer Mean: 3.62 Customer Mean: 4.09\nFrom the histograms and mean calculations, it appears that customers of Blueprinty tend to have a slightly higher average number of patents compared to non-customers. This could suggest a positive impact of using Blueprinty’s software on patent success, though other factors like firm age or region might also influence these results.\nComparing Regions\n\n\n\n\n\n\n\n\n\n\nRegion: There does not seem to be a very distinct pattern that strongly differentiates the regional distribution of customers versus non-customers, although some regions might slightly favor customer presence.\nAge: Customers tend to be younger firms compared to non-customers. This might suggest that younger firms are more likely to adopt Blueprinty’s software, or it could reflect market penetration strategies targeted at newer firms.\n\n\n\n\nIn the case of modeling the number of patents with a Poisson distribution, we start with the assumption that the number of patents awarded (\\(Y\\)) to each firm follows a Poisson distribution. The Poisson distribution is commonly used to model counts of events happening independently over a fixed period or in a fixed area, which fits the context of counting patents awarded over a specific time.\nPoisson Probability Mass Function\nFor a random variable \\(Y\\) that follows a Poisson distribution with a rate parameter \\(\\lambda\\) (average rate of occurrence over an interval), the probability mass function (PMF) is given by: \\[\nf(Y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nWhere: - \\(Y = 0, 1, 2, \\dots\\) is the count of patents (i.e., the number of events).\n\n\\(\\lambda &gt; 0\\) is the mean number of patents awarded per firm over the interval (5 years, in this case).\n\\(e\\) is the base of the natural logarithm.\n\\(Y!\\) is the factorial of \\(Y\\).\n\nLikelihood Function\nThe likelihood function for the Poisson distribution, given a sample of \\(n\\) observed values \\(Y_1, Y_2, \\dots, Y_n\\), is the product of the individual probabilities for each observed value: \\[\nL(\\lambda | Y_1, Y_2, \\dots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThis function indicates how likely it is to observe this particular set of data as a function of \\(\\lambda\\). When we fit a model, we look for the \\(\\lambda\\) that maximizes this likelihood, known as the Maximum Likelihood Estimate (MLE).\nLog-Likelihood\nFor computational convenience, it is often easier to maximize the log of the likelihood because the logarithm is a monotonically increasing function. The log-likelihood of the Poisson distribution is: \\[\n\\log L(\\lambda | Y_1, Y_2, \\dots, Y_n) = \\sum_{i=1}^{n} \\left(-\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!)\\right)\n\\]\nMaximizing this log-likelihood function with respect to \\(\\lambda\\) gives us the MLE of \\(\\lambda\\), which is particularly straightforward in the case of the Poisson distribution since the derivative of the log-likelihood function leads to a simple solution.\nUsing the statsmodels module we can easily implement the above, while taking into account the iscustomer status as a predictor for \\(\\lambda\\).\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\npatents\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1498\n\n\nModel Family:\nPoisson\nDf Model:\n1\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3362.7\n\n\nDate:\nTue, 30 Apr 2024\nDeviance:\n2352.6\n\n\nTime:\n19:24:57\nPearson chi2:\n2.25e+03\n\n\nNo. Iterations:\n4\nPseudo R-squ. (CS):\n0.006565\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n1.2874\n0.015\n88.453\n0.000\n1.259\n1.316\n\n\niscustomer\n0.1215\n0.038\n3.189\n0.001\n0.047\n0.196\n\n\n\n\n\n\nModel Summary:\n\nLog-Likelihood: The log-likelihood of the model is -3362.7, indicating the fit of the model to the data under the log link function.\nPseudo R-squared: The Pseudo R-squared value is 0.006565, which suggests that the model explains a small fraction of the variability in the number of patents.\nCoefficients:\n\nConstant (Intercept): The coefficient for the intercept is 1.2874. This can be interpreted as the log of the expected count of patents for non-customers, which exponentiated gives about \\(e^{1.2874} \\approx 3.62\\) patents on average for non-customers.\nCustomer Status: The coefficient for iscustomer is 0.1215. This coefficient is positive and statistically significant (p-value = 0.001). It suggests that being a customer is associated with a higher rate of patents. Exponentiating this coefficient gives about \\(e^{0.1215} \\approx 1.13\\), which means customers see a 13% increase in the rate of patents relative to non-customers.\n\n\nThis model indicates that there is a statistically significant but relatively modest effect of using Blueprinty’s software on the number of patents received. Customers of Blueprinty, on average, receive about 13% more patents than non-customers, holding other factors constant.\nManual Implementation\nThe formula for the log-likelihood of the Poisson distribution is: \\[\n\\log L(\\lambda | Y) = \\sum_{i=1}^{n} \\left(-\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!)\\right)\n\\]\nIn Python this looks like this:\n-lambda * n + sum(Y_i * log(lambda)) - sum(log(Y_i!))\n\ndef poisson_loglikelihood(lambda_, Y):\n    \"\"\"\n    Calculate the log-likelihood for the Poisson distribution given a lambda and an array of observed counts Y.\n    \n    Args:\n    lambda_ (float): The rate parameter of the Poisson distribution.\n    Y (array-like): Observed counts.\n    \n    Returns:\n    float: The log-likelihood value.\n    \"\"\"\n    return -lambda_ * len(Y) + np.sum(Y * np.log(lambda_)) - np.sum(gammaln(Y + 1))\n\n\nlog_likelihood = poisson_loglikelihood(lambda_, Y)\nlog_likelihood\n\n-3399.0629915336285\n\n\nA log-likelihood of -3399.06 indicates the probability of observing the given patent count data under the assumed Poisson distribution with the computed \\(\\lambda\\). This number by itself is abstract, but it becomes meaningful when used to compare this model against others, such as models with additional variables or different distributions.\nPloting \\(\\lambda\\)\n\nlambda_range = np.linspace(0.5 * lambda_, 1.5 * lambda_, 400)\nlog_likelihood_values = [poisson_loglikelihood(l, Y) for l in lambda_range]\n\nUsing the function created we plot \\(\\lambda\\) on the horizontal axis and the log-likelihood on the vertical axis for the range of lambdas.\n\n\n\n\n\n\n\n\n\n\nCurve Shape: The plot shows a peak around the estimated \\(\\lambda\\) (marked by the red dashed line), which is expected as this value of \\(\\lambda\\) maximizes the log-likelihood according to our model fit. This is characteristic of the behavior of likelihood functions around their maximum values, confirming that our estimation procedure is finding a plausible maximum.\nSensitivity: The curve is relatively steep around the estimated \\(\\lambda\\), indicating that small deviations from this value result in a noticeable decrease in log-likelihood. This suggests that the data provides a clear indication of the rate of patents, supporting the value we estimated.\n\nThe peak of the log-likelihood function tells us the most likely rate of patent awards (given the model and data), with values on either side less likely to produce the observed data.\nExploring MLE of Lambda for a Possion Distribution\nTo do this, we need to differentiate the log-likelihood function with respect to \\(\\lambda\\) and set it equal to zero. The log-likelihood function, given observations \\(Y_1, Y_2, \\dots, Y_n\\), is:\n\\[\n\\log L(\\lambda | Y) = \\sum_{i=1}^{n} \\left(-\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!)\\right)\n\\]\nTaking the derivative of this function with respect to \\(\\lambda\\), we get:\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda | Y) = \\sum_{i=1}^{n} \\left(-1 + \\frac{Y_i}{\\lambda}\\right)\n\\]\nSetting this derivative equal to zero for maximization:\n\\[\n- n + \\sum_{i=1}^{n} \\frac{Y_i}{\\lambda} = 0\n\\]\n\\[\n\\sum_{i=1}^{n} \\frac{Y_i}{\\lambda} = n\n\\]\n\\[\n\\frac{\\sum_{i=1}^{n} Y_i}{\\lambda} = n\n\\]\n\\[\n\\lambda = \\frac{\\sum_{i=1}^{n} Y_i}{n}\n\\]\nThus, \\(\\lambda_{MLE} = \\bar{Y}\\), where \\(\\bar{Y}\\) is the sample mean of the observed values \\(Y\\). This result aligns with our intuition and theoretical understanding, as \\(\\lambda\\) in a Poisson distribution represents both the mean and the variance of the distribution.\n\nlambda_mle = Y.mean()\nlambda_mle\n\n3.6846666666666668\n\n\nThe Maximum Likelihood Estimate (MLE) for lambda based on the observed data is approximately 3.685. This is the average number of patents per firm over the last 5 years, which indeed serves as the MLE of lambda in our Poisson model. This result confirms our theoretical derivation and reflects the natural interpretation of lambda in the Poisson distribution as the average rate (or mean) of occurrence of the event (patents awarded).\nFinding the MLE with scipy.optimize\n\ndef poisson_loglikelihood_v2(lambda_, Y):\n    \"\"\"\n    Calculate the negative log-likelihood for the Poisson distribution given a lambda and an array of observed counts Y.\n    \n    Args:\n    lambda_ (float): The rate parameter of the Poisson distribution.\n    Y (array-like): Observed counts.\n    \n    Returns:\n    float: The negative log-likelihood value.\n    \"\"\"\n    if lambda_ &lt;= 0:  # To ensure lambda is positive\n        return np.inf\n    return -(-lambda_ * len(Y) + np.sum(Y * np.log(lambda_)) - np.sum(gammaln(Y + 1)))\n\nUpdated version of the function above to make the absolute positive.\n\nfrom scipy.optimize import minimize\nresult = minimize(poisson_loglikelihood_v2, initial_lambda, args=(Y,), method='L-BFGS-B', bounds=[(0.001, None)])\nresult\n\n  message: CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH\n  success: True\n   status: 0\n      fun: 3367.6837722351047\n        x: [ 3.685e+00]\n      nit: 8\n      jac: [-2.728e-04]\n     nfev: 18\n     njev: 9\n hess_inv: &lt;1x1 LbfgsInvHessProduct with dtype=float64&gt;\n\n\nUsing minimize() finds the MLE of the lambda parameter for the given poisson distribution that best fits the observed data.\n\nEstimated lambda: Approximately 3.685, which closely matches the sample mean we calculated manually and confirms our previous findings.\nOptimization Status: The process converged successfully, indicating that a minimum of the negative log-likelihood function was found effectively.\nFunction Evaluations: It took 18 function evaluations to reach convergence, showing an efficient optimization given the simplicity of the function.\nLog-Likelihood: The minimum value of the log-likelihood at the optimum lambda is about 3367.684.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    \"\"\"\n    Calculate the negative log-likelihood for the Poisson regression model given beta coefficients, observed counts Y, and covariate matrix X.\n    \n    Args:\n    beta (array-like): The coefficient vector for the covariates.\n    Y (array-like): Observed counts.\n    X (2D array-like): Covariate matrix where each row corresponds to an observation and each column to a covariate.\n    \n    Returns:\n    float: The negative log-likelihood value.\n    \"\"\"\n    lambda_i = np.exp(X @ beta)\n    log_likelihood = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n    return -log_likelihood\n\nThis updated function allows us to pass the covariate matrix X.\nBeta Coefficients: The estimated coefficients for the model are as follows: - Intercept: beta_0 approx 1.215\n\nAge: beta_1 approx 1.046\nAge Squared: beta_2 approx -1.141\nCustomer Status: beta_3 approx 0.118\nRegion Northeast: beta_4 approx 0.099\nRegion Northwest: beta_5 approx -0.020\nRegion South: beta_6 approx 0.057\nRegion Southwest: beta_7 approx 0.051\nAge Effects: The positive coefficient for age and the negative coefficient for age squared suggest a non-linear relationship where the number of patents increases with age to a point, after which it begins to decrease. This could represent a “peak productive period” for firms.\nCustomer Status: The positive coefficient for being a customer indicates that, all else equal, firms using Blueprinty’s software are expected to have a higher rate of patents.\nRegional Effects: The coefficients for the regions indicate slight variations in patent rates across different regions, with some regions showing slightly higher or lower rates than the baseline (Midwest, which was dropped due to one-hot encoding).\n\nThis model provides a richer understanding of the factors influencing patent rates across firms and confirms the utility of including additional covariates to model such outcomes more accurately.\nHessian of Poisson Model with Covariats\nThe Hessian matrix is a fundamental component in the inference of maximum likelihood estimators. By calculating the inverse of the Hessian matrix at the optimum (MLEs of the parameters), we obtain the covariance matrix of the estimates. The diagonal elements of this matrix provide the variances (and thus standard errors) of the parameter estimates.\nThe Hessian also helps confirm the stability and reliability of the optimization process. A well-behaved Hessian (i.e., positive definite at the optimum) suggests that the optimization found a true maximum of the likelihood function. This is crucial for decision making and making statistical inferences.\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStandard Error\nStandard Error (from Hessian)\n\n\n\n\nintercept\n1.215451\n31.305234\n0.036425\n\n\nage\n1.046434\n57.308666\n0.100487\n\n\nage_squared\n-1.140819\n57.254890\n0.102494\n\n\niscustomer\n0.118172\n10.565816\n0.038920\n\n\nregion_Northeast\n0.098560\n23.819037\n0.042007\n\n\nregion_Northwest\n-0.020052\n3.153481\n0.053782\n\n\nregion_South\n0.057158\n9.626092\n0.052675\n\n\nregion_Southwest\n0.051278\n13.110999\n0.047213\n\n\n\n\n\n\n\n\nCoefficient:\n\nDescription: This column shows the estimated values of the regression coefficients (()) for each predictor in the model. These values represent the log change in the expected count of patents for a one-unit increase in the predictor, holding other variables constant.\nInterpretation:\n\nPositive values indicate that an increase in the predictor is associated with an increase in the expected count of patents.\nNegative values indicate that an increase in the predictor leads to a decrease in the expected count of patents.\nFor binary predictors like iscustomer, the coefficient tells us the log change in the expected count of patents when the firm is a customer versus not a customer.\n\n\nInitial Standard Error:\n\nDescription: Initially, the standard errors were calculated from an approximation of the covariance matrix obtained indirectly through the optimization process.\nInterpretation: Standard errors measure the variability or uncertainty in the coefficient estimates. Larger standard errors suggest greater uncertainty about the coefficient estimate.\n\nStandard Error (from Hessian):\n\nDescription: These standard errors are calculated directly from the Hessian matrix of the negative log-likelihood function at the estimated coefficients. The Hessian provides a second-order approximation to the curvature of the likelihood surface at the optimum.\nInterpretation: Like the initial standard errors, these values measure the precision of the estimates. The smaller these values, the more confident we can be about the accuracy of the estimated coefficients.\n\nValidating Results with Statsmodels glm()\nUp to this point in the analysis, we took a “looking under the hood” approach to this case study. Using the open source statsmodels api we can easily summarize our work.\n\nModel Coefficients\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient (Manual)\nStandard Error (Manual)\nCoefficient (GLM)\nStandard Error (GLM)\n\n\n\n\nIntercept\n1.215\n0.036\n1.215\n0.036\n\n\nAge\n1.046\n0.100\n1.046\n0.100\n\n\nAge Squared\n-1.141\n0.102\n-1.141\n0.102\n\n\nIs Customer\n0.118\n0.039\n0.118\n0.039\n\n\nRegion Northeast\n0.099\n0.042\n0.099\n0.042\n\n\nRegion Northwest\n-0.020\n0.054\n-0.020\n0.054\n\n\nRegion South\n0.057\n0.053\n0.057\n0.053\n\n\nRegion Southwest\n0.051\n0.047\n0.051\n0.047\n\n\n\n\nConsistency: The coefficients and standard errors from the statsmodels.GLM function match exactly with those computed manually using the Hessian-based approach. This indicates the correctness of our manual calculations and the model setup.\nStatistical Significance: The z-values and P&gt;|z| columns from the statsmodels output provide tests for the significance of each predictor. Coefficients with a lower p-value are statistically significant at common significance levels (e.g., 0.05), confirming the impact of those variables on the response variable.\nAge and Age Squared: Both coefficients are statistically significant with expected signs, indicating a peak effect of age on patent production.\nIs Customer: The positive coefficient for this variable is statistically significant, indicating that customers of Blueprinty are likely to have higher patent counts.\nRegions: Differences among regions show varying levels of significance, with the Northeast being significant, suggesting regional variations in patent counts.\n\n\n\n\nEffect of Using Blueprinty’s Software:\n\nThe coefficient for the iscustomer variable is positive and statistically significant. This suggests that firms using Blueprinty’s software, on average, have higher rates of patent awards compared to those that do not use the software.\nQuantitatively, the coefficient value (approximately 0.118) indicates that being a customer of Blueprinty is associated with an increase in the expected log count of patents. This translates into an exponential effect, suggesting that customers see about a 12.5% increase in the expected number of patents ([exp(0.118) - 1] * 100%).\n\nAge of Firm:\n\nThe relationship between the age of a firm and its patent success is modeled as a quadratic function, with both age and age_squared being statistically significant. This indicates a peak effect where the number of patents increases with age up to a point, after which it starts to decline. This could reflect increasing expertise and resources over time, followed by a plateau or decline as firms age further.\n\nRegional Variations:\n\nThere are significant regional differences in patent success rates. For example, firms in the Northeast show a significantly higher rate of patent success compared to the baseline region (Midwest), suggesting regional disparities in innovation or business environments that could affect patent outcomes.\n\nConclusions:\n\nBlueprinty’s Software: The analysis strongly supports the marketing claim that Blueprinty’s software helps increase the success rate of patent applications. Firms using Blueprinty’s software are statistically more likely to have higher patent counts, which could be attributed to the efficiency and effectiveness of the software in managing patent applications.\nPolicy and Strategy Implications: For Blueprinty, these findings justify strategies aimed at expanding their customer base by highlighting the positive impact of their software on patent success. Additionally, they might consider tailored marketing strategies for different regions or age groups of firms based on the differential effects observed.\nLimitations and Further Research: While the results are promising, they are based on observational data, which can be prone to confounding factors not controlled in the model. Further research could involve more detailed data, potentially longitudinal studies, to better isolate the effect of the software from other factors. Additionally, exploring other variables like industry type, size of the firm, or specific features used in the software could provide deeper insights."
  },
  {
    "objectID": "projects/Assignment2/hw2_questions.html#airbnb-case-study",
    "href": "projects/Assignment2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction & Data Exploration\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\n\n\n\n\nInteractive Airbnb Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\nLoading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n40628.000000\n4.062800e+04\n40628.000000\n40468.000000\n40552.000000\n40628.000000\n40628.000000\n30433.000000\n30374.000000\n30372.000000\n\n\nmean\n20314.500000\n9.698889e+06\n1102.368219\n1.124592\n1.147046\n144.760732\n15.904426\n9.198370\n9.413544\n9.331522\n\n\nstd\n11728.437705\n5.460166e+06\n1383.269358\n0.385884\n0.691746\n210.657597\n29.246009\n1.119935\n0.844949\n0.902966\n\n\nmin\n1.000000\n2.515000e+03\n1.000000\n0.000000\n0.000000\n10.000000\n0.000000\n2.000000\n2.000000\n2.000000\n\n\n25%\n10157.750000\n4.889868e+06\n542.000000\n1.000000\n1.000000\n70.000000\n1.000000\n9.000000\n9.000000\n9.000000\n\n\n50%\n20314.500000\n9.862878e+06\n996.000000\n1.000000\n1.000000\n100.000000\n4.000000\n10.000000\n10.000000\n10.000000\n\n\n75%\n30471.250000\n1.466789e+07\n1535.000000\n1.000000\n1.000000\n170.000000\n17.000000\n10.000000\n10.000000\n10.000000\n\n\nmax\n40628.000000\n1.800967e+07\n42828.000000\n8.000000\n10.000000\n10000.000000\n421.000000\n10.000000\n10.000000\n10.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring null values\n\n\n\n\n\n\n\nUnnamed: 0                       0\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\n\n\n\nDescriptive Statistics:\n\nCount: 40,628 listings, except for a few variables with missing values.\nDates: The dataset was last scraped on 2 dates, with the majority on April 2, 2017.\nRoom Type: Three types are available: Entire home/apt, Private room, Shared room (most listings are Entire home/apt).\nBathrooms: Ranges from 0 to 8, with an average of about 1.12 bathrooms per listing.\nBedrooms: Ranges from 0 to 10, with an average of about 1.15 bedrooms per listing.\nPrice: Ranges from $10 to $10,000 per night, with a median price of $100.\nNumber of Reviews: Ranges from 0 to 421, with a mean of approximately 15.9 reviews per listing.\n\nMissing Values:\n\nHost Since: 35 missing values.\nBathrooms: 160 missing values.\nBedrooms: 76 missing values.\nReview Scores: Considerable number of missing entries in review scores (cleanliness, location, and value), with over 10,000 missing values in each category.\n\nHandling Missing Values\nBefore proceeding with a regression model, it’s essential to handle the missing values, particularly in the review_scores_* and bathrooms columns, as these could be important predictors for our model. To manage these missing values we do the following:\n\nDrop rows: where the missing values are likely to bias the results significantly.\nImputation: using mean, median, or another method depending on the distribution.\n\n\n\nPoisson Regression\nAs mentioned earlier prior to any form of regression analysis, the data must be cleaned. Here we dropped the reviews columns and imputed the few missing values.\n\n\n\n\n\n\nInteractive “Cleaned” Airbnb Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\ninstant_bookable\n\n\n\n\nLoading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhost_since: Imputed using forward fill method, as it’s a date and the forward fill helps preserve the chronological order.\nbathrooms and bedrooms: Missing values were filled using the median values of their respective columns, ensuring that the typical property characteristics are maintained.\n\nThe dataset is now complete with no missing values across all columns.\nPoisson Regression\n\nformula = 'number_of_reviews ~ days + room_type + bathrooms + bedrooms + price + instant_bookable'\nmodel = glm(formula=formula, data=data_cleaned, family=sm.families.Poisson()).fit()\nmodel.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nnumber_of_reviews\nNo. Observations:\n40628\n\n\nModel:\nGLM\nDf Residuals:\n40620\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-7.1526e+05\n\n\nDate:\nTue, 30 Apr 2024\nDeviance:\n1.3065e+06\n\n\nTime:\n19:24:57\nPearson chi2:\n2.10e+06\n\n\nNo. Iterations:\n11\nPseudo R-squ. (CS):\n0.5694\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n2.7778\n0.004\n624.334\n0.000\n2.769\n2.787\n\n\nroom_type[T.Private room]\n-0.1484\n0.003\n-52.321\n0.000\n-0.154\n-0.143\n\n\nroom_type[T.Shared room]\n-0.4054\n0.009\n-47.041\n0.000\n-0.422\n-0.389\n\n\ndays\n5.012e-05\n3.52e-07\n142.264\n0.000\n4.94e-05\n5.08e-05\n\n\nbathrooms\n-0.1085\n0.004\n-28.301\n0.000\n-0.116\n-0.101\n\n\nbedrooms\n0.0989\n0.002\n49.371\n0.000\n0.095\n0.103\n\n\nprice\n-0.0005\n1.23e-05\n-38.983\n0.000\n-0.001\n-0.000\n\n\ninstant_bookable\n0.3744\n0.003\n130.444\n0.000\n0.369\n0.380\n\n\n\n\n\n\nModel Coefficients Interpretation:\n\nIntercept: The base log-count of reviews is approximately 2.778. (or roughly two reviews per listing in this scenario)\nRoom Type:\n\nPrivate Room: Listings that are private rooms have about 14.84% fewer reviews than entire homes/apartments, holding all else constant.\nShared Room: Listings that are shared rooms have about 40.54% fewer reviews than entire homes/apartments, holding all else constant.\n\nDays: A unit increase in the number of days a listing has been posted increases the expected count of reviews by about 0.005%.\nBathrooms: Each additional bathroom is associated with about 10.85% fewer reviews, holding all else constant.\nBedrooms: Each additional bedroom is associated with about 9.89% more reviews, holding all else constant.\nPrice: Each additional dollar in price is associated with about 0.05% fewer reviews, holding all else constant.\nInstant Bookable: Listings that are instant bookable are expected to have about 37.44% more reviews than those that are not, holding all else constant."
  },
  {
    "objectID": "projects/Assignment3/hw3_questions.html",
    "href": "projects/Assignment3/hw3_questions.html",
    "title": "Multinomial Logit Examples",
    "section": "",
    "text": "In this article we use the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans."
  },
  {
    "objectID": "projects/Assignment3/hw3_questions.html#estimating-yogurt-preferences",
    "href": "projects/Assignment3/hw3_questions.html#estimating-yogurt-preferences",
    "title": "Multinomial Logit Examples",
    "section": "1. Estimating Yogurt Preferences",
    "text": "1. Estimating Yogurt Preferences\n\nLikelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, size, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\n\n\nYogurt Dataset\nWe will use the yogurt_data dataset, which provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc.\n\n\n\n\n\n\nInteractive Raw Yogurt Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\n\n\n\n\nLoading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet the vector of product features include brand dummy variables for yogurts 1-3 (we’ll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts’ prices:\n\\[\nx_j' = [\\mathbf{1}(\\text{Yogurt 1}), \\mathbf{1}(\\text{Yogurt 2}), \\mathbf{1}(\\text{Yogurt 3}), X_f, X_p]\n\\]\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)).\nWhat we would like to do is reorganize the data from a “wide” shape with \\(n\\) rows and multiple columns for each covariate, to a “long” shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we’ll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be “pivoted” or “melted” from wide to long.\n\n\n\n\n\n\nInteractive Cleaned Yogurt Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nproduct\nchosen\nfeatured\nprice\n\n\n\n\nLoading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimation\n\n\n\n\n\n\nlog_likelihood()\n\n\n\n\n\n\ndef log_likelihood(beta, data):\n    \"\"\"\n    Calculate the log-likelihood of the MNL model.\n\n    Parameters:\n    beta (array): Array of coefficients [β1, β2, β3, βf, βp].\n    data (DataFrame): The reshaped long format data with columns ['id', 'product', 'chosen', 'featured', 'price'].\n\n    Returns:\n    float: The log-likelihood value.\n    \"\"\"\n    beta1, beta2, beta3, beta_f, beta_p = beta\n    data['yogurt1'] = (data['product'] == 1).astype(int)\n    data['yogurt2'] = (data['product'] == 2).astype(int)\n    data['yogurt3'] = (data['product'] == 3).astype(int)\n    data['utility'] = (beta1 * data['yogurt1'] + \n                       beta2 * data['yogurt2'] + \n                       beta3 * data['yogurt3'] + \n                       beta_f * data['featured'] + \n                       beta_p * data['price'])\n    data['exp_utility'] = np.exp(data['utility'])\n    data['sum_exp_utility'] = data.groupby('id')['exp_utility'].transform('sum')\n    data['probability'] = data['exp_utility'] / data['sum_exp_utility']\n    data['log_likelihood'] = data['chosen'] * np.log(data['probability'])\n    return -data['log_likelihood'].sum()\n\n\n\n\n\ninitial_beta = np.zeros(5)\nlog_likelihood(initial_beta, yogurt_long)\n\n3368.6952975213344\n\n\nUsing optim() in R or optimize() in Python to find the MLEs for the 5 parameters (\\(\\beta_1, \\beta_2, \\beta_3, \\beta_f, \\beta_p\\)).\n\nresult = minimize(log_likelihood, initial_beta, args=(yogurt_long,), method='BFGS')\nestimated_beta = result.x\nestimated_beta\n\narray([  1.38775332,   0.64350491,  -3.08611501,   0.48741354,\n       -37.05792291])\n\n\n\n\nDiscussion\nWe learn the following…\n\nProduct Intercepts:\n\n(\\(\\beta_1\\)) (Yogurt 1): The positive coefficient suggests that Yogurt 1 is relatively preferred.\n(\\(\\beta_2\\)) (Yogurt 2): This positive coefficient also indicates a relative preference for Yogurt 2, but it’s lower than Yogurt 1.\n(\\(\\beta_3\\)) (Yogurt 3): The negative coefficient suggests that Yogurt 3 is less preferred compared to the omitted category (Yogurt 4).\n\nFeatured (\\(\\beta_f\\)): The positive coefficient 0.487 implies that featuring a yogurt increases its utility and thus its probability of being chosen.\nPrice (\\(\\beta_p\\)): The large negative coefficient -37.058 indicates a strong negative effect of price on the utility, meaning higher prices significantly reduce the likelihood of a yogurt being chosen.\n\nUsing the estimated price coefficient as a dollar-per-util conversion to calculate the dollar benefit between the most-preferred yogurt (the one with the highest intercept) and the least preferred yogurt (the one with the lowest intercept). This is a per-unit monetary measure of brand value.\n\nconversion_factor = -1 / estimated_beta[4]\nutility_difference = estimated_beta[0] - estimated_beta[2]\nmonetary_value = utility_difference * conversion_factor\nmonetary_value\n\n0.12072636520970716\n\n\n❗ The monetary benefit between the most-preferred yogurt (Yogurt 1) and the least-preferred yogurt (Yogurt 3) is approximately $0.12 per unit. This means consumers value Yogurt 1 about $0.12 more per unit than Yogurt 3, based on the estimated utilities. ❗\n\nOne benefit of the MNL model is that we can simulate counterfactuals (eg, what if the price of yogurt 1 was $0.10/oz instead of $0.08/oz).\nCalculating market shares in the market at the time the data were collected. Then, increasing the price of yogurt 1 by $0.10 and using the fitted model to predict p(y|x) for each consumer and each product.\n\n\n\n\n\n\npredict_market_shares()\n\n\n\n\n\n\ndef predict_market_shares(beta, data):\n    \"\"\"\n    Predict market shares using the estimated beta coefficients.\n\n    Parameters:\n    beta (array): Array of coefficients [β1, β2, β3, βf, βp].\n    data (DataFrame): The reshaped long format data with columns ['id', 'product', 'chosen', 'featured', 'price'].\n\n    Returns:\n    DataFrame: The predicted market shares for each product.\n    \"\"\"\n    data['yogurt1'] = (data['product'] == 1).astype(int)\n    data['yogurt2'] = (data['product'] == 2).astype(int)\n    data['yogurt3'] = (data['product'] == 3).astype(int)\n    data['utility'] = (beta[0] * data['yogurt1'] + \n                       beta[1] * data['yogurt2'] + \n                       beta[2] * data['yogurt3'] + \n                       beta[3] * data['featured'] + \n                       beta[4] * data['price'])\n    data['exp_utility'] = np.exp(data['utility'])\n    data['sum_exp_utility'] = data.groupby('id')['exp_utility'].transform('sum')\n    data['probability'] = data['exp_utility'] / data['sum_exp_utility']\n    market_shares = data.groupby('product')['probability'].mean().reset_index()\n    market_shares.columns = ['product', 'market_share']\n    return market_shares\n\n\n\n\n\n\n(   product  market_share\n 0        1      0.341975\n 1        2      0.401235\n 2        3      0.029218\n 3        4      0.227572,\n    product  market_share\n 0        1      0.021118\n 1        2      0.591145\n 2        3      0.044040\n 3        4      0.343697)\n\n\nMarket Shares Analysis\nThe market shares before and after the price increase of Yogurt 1 are as follows:\nOriginal Market Shares:\n\nYogurt 1: 34.20%\nYogurt 2: 40.12%\nYogurt 3: 2.92%\nYogurt 4: 22.76%\n\nAdjusted Market Shares (after $0.10 price increase for Yogurt 1):\n\nYogurt 1: 2.11%\nYogurt 2: 59.11%\nYogurt 3: 4.40%\nYogurt 4: 34.37%\n\nIncreasing the price of Yogurt 1 by $0.10 drastically decreases its market share from 34.20% to 2.11%. Meanwhile, the market shares for Yogurt 2, Yogurt 3, and Yogurt 4 increase, with Yogurt 2 seeing the most significant rise from 40.12% to 59.11%."
  },
  {
    "objectID": "projects/Assignment3/hw3_questions.html#estimating-minivan-preferences",
    "href": "projects/Assignment3/hw3_questions.html#estimating-minivan-preferences",
    "title": "Multinomial Logit Examples",
    "section": "2. Estimating Minivan Preferences",
    "text": "2. Estimating Minivan Preferences\n\nData\n\n\n\n\n\n\nInteractive Conjoint Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nresp.id\nques\nalt\ncarpool\nseat\ncargo\neng\nprice\nchoice\n\n\n\n\nLoading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCojoint Variables\n\n\n\n\n\n\nresp.id: Respondent identifier.\nques: Choice task number.\nalt: Alternative number within the choice task.\ncarpool: Carpool option (yes/no).\nseat: Number of seats (6, 7, 8).\ncargo: Cargo space (2ft, 3ft).\neng: Engine type (gas, hybrid).\nprice: Price in thousands of dollars.\nchoice: Indicator for whether the alternative was chosen (1 if chosen, 0 otherwise).\n\n\n\n\nThe attributes (levels) were number of seats (6,7,8), cargo space (2ft, 3ft), engine type (gas, hybrid, electric), and price (in thousands of dollars).\n\nNumber of respondents: 200\nNumber of choice tasks per respondent: 15\nNumber of alternatives presented in each choice task: 3\n\nEach respondent in the survey completed 15 choice tasks, with each task presenting 3 different alternatives to choose from.\n\n\nModel\nWe’ll estimate an MNL model omitting the following levels to avoid multicollinearity:\n\n6 seats\n2ft cargo\nGas engine\n\nThe variables we will include in our model are:\n\nseat_7: Dummy variable for 7 seats.\nseat_8: Dummy variable for 8 seats.\ncargo_3ft: Dummy variable for 3ft cargo space.\neng_hyb: Dummy variable for hybrid engine.\nprice: Continuous variable for price in thousands of dollars.\n\n\n\n\n\n\n\nconjoint_log_likelihood()\n\n\n\n\n\n\ndef conjoint_log_likelihood(beta, data):\n    \"\"\"\n    Calculate the log-likelihood of the MNL model for the conjoint data.\n\n    Parameters:\n    beta (array): Array of coefficients [β_seat_7, β_seat_8, β_cargo_3ft, β_eng_hyb, β_price].\n    data (DataFrame): The conjoint data with dummy variables.\n\n    Returns:\n    float: The log-likelihood value.\n    \"\"\"\n    beta_seat_7, beta_seat_8, beta_cargo_3ft, beta_eng_hyb, beta_price = beta\n    data['utility'] = (beta_seat_7 * data['seat_7'] +\n                       beta_seat_8 * data['seat_8'] +\n                       beta_cargo_3ft * data['cargo_3ft'] +\n                       beta_eng_hyb * data['eng_hyb'] +\n                       beta_price * data['price'])\n    data['exp_utility'] = np.exp(data['utility'])\n    data['sum_exp_utility'] = data.groupby(['resp.id', 'ques'])['exp_utility'].transform('sum')\n    data['probability'] = data['exp_utility'] / data['sum_exp_utility']\n    data['log_likelihood'] = data['choice'] * np.log(data['probability'])\n    return -data['log_likelihood'].sum()\n\n\n\n\n\ninitial_beta_conjoint = np.zeros(5)\nconjoint_result = minimize(conjoint_log_likelihood, initial_beta_conjoint, args=(conjoint_data,), method='BFGS')\nestimated_beta_conjoint = conjoint_result.x\nestimated_beta_conjoint\n\narray([-0.48592307, -0.28346544,  0.41191849, -0.10548881, -0.15573405])\n\n\nThe estimated coefficients for the MNL model are as follows:\n\nbeta seat 7: -0.486\nbeta seat 8: -0.283\nbeta cargo 3ft: 0.412\nbeta hybrid engine: -0.105\nbeta sprice: -0.156\n\n\n\nResults\n\nSeats:\n\n(7 seats): The negative coefficient suggests that 7 seats are less preferred compared to the baseline category (6 seats).\n(8 seats): The negative coefficient suggests that 8 seats are also less preferred compared to 6 seats, but less so than 7 seats.\n\nCargo Space:\n\n(3ft cargo): The positive coefficient indicates that 3ft of cargo space is preferred over 2ft of cargo space.\n\nEngine:\n\n(Hybrid Engine): The negative coefficient suggests that hybrid engines are less preferred compared to gas engines.\n\nPrice:\n\n(Price): The negative coefficient indicates that higher prices decrease the utility of the minivan, making it less likely to be chosen.\n\n\n\nconversion_factor_conjoint = -1 / estimated_beta_conjoint[4]\nutility_difference_cargo = estimated_beta_conjoint[2]\nmonetary_value_cargo = utility_difference_cargo * conversion_factor_conjoint\nmonetary_value_cargo\n\n2.645012364801245\n\n\nThe dollar value of having 3ft of cargo space compared to 2ft of cargo space is approximately $2,645. This means that, on average, consumers value the additional cargo space at $2,645.\nLet’s assume the market consists of the following 6 minivans.\n\n\n\nMinivan\nSeats\nCargo\nEngine\nPrice\n\n\n\n\nA\n7\n2\nHyb\n30\n\n\nB\n6\n2\nGas\n30\n\n\nC\n8\n2\nGas\n30\n\n\nD\n7\n3\nGas\n40\n\n\nE\n6\n2\nElec\n40\n\n\nF\n7\n2\nHyb\n35\n\n\n\nWe will use the estimated model to predict the market shares of these six minivans.\n\n\n\n\n\n\nCode block\n\n\n\n\n\n\nmarket_configurations = pd.DataFrame({\n    'minivan': ['A', 'B', 'C', 'D', 'E', 'F'],\n    'seat': [7, 6, 8, 7, 6, 7],\n    'cargo': ['2ft', '2ft', '2ft', '3ft', '2ft', '2ft'],\n    'eng': ['hyb', 'gas', 'gas', 'gas', 'elec', 'hyb'],\n    'price': [30, 30, 30, 40, 40, 35]\n})\nmarket_configurations['seat_7'] = (market_configurations['seat'] == 7).astype(int)\nmarket_configurations['seat_8'] = (market_configurations['seat'] == 8).astype(int)\nmarket_configurations['cargo_3ft'] = (market_configurations['cargo'] == '3ft').astype(int)\nmarket_configurations['eng_hyb'] = (market_configurations['eng'] == 'hyb').astype(int)\nmarket_configurations['utility'] = (estimated_beta_conjoint[0] * market_configurations['seat_7'] +\n                                    estimated_beta_conjoint[1] * market_configurations['seat_8'] +\n                                    estimated_beta_conjoint[2] * market_configurations['cargo_3ft'] +\n                                    estimated_beta_conjoint[3] * market_configurations['eng_hyb'] +\n                                    estimated_beta_conjoint[4] * market_configurations['price'])\nmarket_configurations['exp_utility'] = np.exp(market_configurations['utility'])\nmarket_configurations['market_share'] = market_configurations['exp_utility'] / market_configurations['exp_utility'].sum()\n\n\n\n\nNote: Our professor took this example from the “R 4 Marketing Research” book by Chapman and Feit. 🙂\nThe predicted market shares for the six minivan configurations are as follows:\n\n\n\nMinivan\nMarket Share\n\n\n\n\nA\n18.66%\n\n\nB\n33.70%\n\n\nC\n25.38%\n\n\nD\n6.59%\n\n\nE\n7.10%\n\n\nF\n8.56%\n\n\n\n\nMinivan B (6 seats, 2ft cargo, gas engine, $30k) has the highest predicted market share at 33.70%.\nMinivan C (8 seats, 2ft cargo, gas engine, $30k) and Minivan A (7 seats, 2ft cargo, hybrid engine, $30k) also have substantial market shares at 25.38% and 18.66%, respectively.\nMinivans with higher prices or different engine types (like hybrid or electric) tend to have lower market shares."
  },
  {
    "objectID": "projects/Assignment4/hw4_questions.html",
    "href": "projects/Assignment4/hw4_questions.html",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "This post implements a few measure of variable importance, interpreted as a key drivers analysis, for certain aspects of a payment card on customer satisfaction with that payment card.\n\n🚧 Work in progress 👷"
  }
]